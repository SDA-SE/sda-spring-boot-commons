{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SDA Spring Boot Commons \u00b6 A set of libraries to bootstrap Spring Boot services easily that follow the patterns and specifications promoted by the SDA SE. Spring Boot 3 \ud83e\udd73 Upgrade to Spring Boot 3 is released with version 3.0.0 . \ud83e\udd73 Please follow the migration guide to upgrade. Features \u00b6 Starter Description sda-commons-starter-web Provides the required features for an SDA-compliant microservice including OIDC authentication, OPA authorization, health checks, OpenTelemetry, Prometheus metrics and hardening the service . sda-commons-starter-mongodb Provides default configuration based on the org.springframework.boot:spring-boot-starter-data-mongodb . sda-commons-starter-kafka Provides default producer und consumer configuration based on org.springframework.kafka:spring-kafka . sda-commons-starter-s3 Provides features for dealing with the Amazon S3 file storage. sda-commons-asyncapi Provides utilities to create AsyncAPI schemas, usually used as test dependency to build the schema in a test. The provided documentation aims to provide SDA-specific information. All other information are referenced in the Spring and Spring Boot documentation . Getting started \u00b6 Each starter is provided as an isolated library and relies on the Spring Boot and Spring Cloud dependency management. Most of the starters themselves include some starters provided by Spring or the community. When using any sda-spring-boot-commons starter make sure to include the provided dependency management. The provided dependency management is based on the Spring Boot and Spring Cloud dependency management to align transitive dependencies with the provided Spring versions. 1 2 3 4 5 6 7 8 9 10 11 project . ext { sdaSpringCommonsVersions = '0.11.0' } dependencies { implementation enforcedPlatform ( \"org.sdase.commons.spring.boot:sda-commons-dependencies:$sdaSpringCommonsVersions\" ) implementation enforcedPlatform ( \"org.sdase.commons.spring.boot:sda-commons-bom:$sdaSpringCommonsVersions\" ) implementation 'org.sdase.commons.spring.boot:sda-commons-starter-web' testImplementation \"org.sdase.commons.spring.boot:sda-commons-web-testing\" } Artifacts of SDA Spring Boot Commons are available at Maven Central since release 0.11.2 . Static directories \u00b6 Since Spring Boot runs in an embedded Tomcat server, it needs some tmp directories to support the container run in a readonly file system. Therefore, your application need to set a folder called static and a folder structure tmp/tomcat in the root directory of your container. In order to do so, create a folder static and tmp/tomcat on src/main/jib . If you use non-root docker images, your jib config in your build.gradle file must include container.workingDirectory='/' , so jib will use the root folder to create the sub folders, e.g: 1 2 3 4 jib { from.image = 'gcr.io/distroless/java17-debian11:nonroot' container.workingDirectory='/' } When the container's image is generated with gradlew jibDockerBuild , these folders will be copied to the container. In case you need the tmp folder to be writable, you can mount a volume in your container. The default path is /tmp/tomcat , but you can overwrite it setting the environment variable pointing to your folder: 1 SERVER_TOMCAT_BASEDIR=/path-to-your-folder","title":"Overview"},{"location":"#sda-spring-boot-commons","text":"A set of libraries to bootstrap Spring Boot services easily that follow the patterns and specifications promoted by the SDA SE. Spring Boot 3 \ud83e\udd73 Upgrade to Spring Boot 3 is released with version 3.0.0 . \ud83e\udd73 Please follow the migration guide to upgrade.","title":"SDA Spring Boot Commons"},{"location":"#features","text":"Starter Description sda-commons-starter-web Provides the required features for an SDA-compliant microservice including OIDC authentication, OPA authorization, health checks, OpenTelemetry, Prometheus metrics and hardening the service . sda-commons-starter-mongodb Provides default configuration based on the org.springframework.boot:spring-boot-starter-data-mongodb . sda-commons-starter-kafka Provides default producer und consumer configuration based on org.springframework.kafka:spring-kafka . sda-commons-starter-s3 Provides features for dealing with the Amazon S3 file storage. sda-commons-asyncapi Provides utilities to create AsyncAPI schemas, usually used as test dependency to build the schema in a test. The provided documentation aims to provide SDA-specific information. All other information are referenced in the Spring and Spring Boot documentation .","title":"Features"},{"location":"#getting-started","text":"Each starter is provided as an isolated library and relies on the Spring Boot and Spring Cloud dependency management. Most of the starters themselves include some starters provided by Spring or the community. When using any sda-spring-boot-commons starter make sure to include the provided dependency management. The provided dependency management is based on the Spring Boot and Spring Cloud dependency management to align transitive dependencies with the provided Spring versions. 1 2 3 4 5 6 7 8 9 10 11 project . ext { sdaSpringCommonsVersions = '0.11.0' } dependencies { implementation enforcedPlatform ( \"org.sdase.commons.spring.boot:sda-commons-dependencies:$sdaSpringCommonsVersions\" ) implementation enforcedPlatform ( \"org.sdase.commons.spring.boot:sda-commons-bom:$sdaSpringCommonsVersions\" ) implementation 'org.sdase.commons.spring.boot:sda-commons-starter-web' testImplementation \"org.sdase.commons.spring.boot:sda-commons-web-testing\" } Artifacts of SDA Spring Boot Commons are available at Maven Central since release 0.11.2 .","title":"Getting started"},{"location":"#static-directories","text":"Since Spring Boot runs in an embedded Tomcat server, it needs some tmp directories to support the container run in a readonly file system. Therefore, your application need to set a folder called static and a folder structure tmp/tomcat in the root directory of your container. In order to do so, create a folder static and tmp/tomcat on src/main/jib . If you use non-root docker images, your jib config in your build.gradle file must include container.workingDirectory='/' , so jib will use the root folder to create the sub folders, e.g: 1 2 3 4 jib { from.image = 'gcr.io/distroless/java17-debian11:nonroot' container.workingDirectory='/' } When the container's image is generated with gradlew jibDockerBuild , these folders will be copied to the container. In case you need the tmp folder to be writable, you can mount a volume in your container. The default path is /tmp/tomcat , but you can overwrite it setting the environment variable pointing to your folder: 1 SERVER_TOMCAT_BASEDIR=/path-to-your-folder","title":"Static directories"},{"location":"asyncapi/","text":"sda-commons-async-api \u00b6 Experimental Please be aware that SDA SE is likely to change or remove this artifact in the future This module contains the AsyncApiGenerator to generate AsyncAPI specs from a template and model classes in a code first approach. The AsyncAPI specification is the industry standard for defining asynchronous APIs. Usage \u00b6 If the code first approach is used to create an AsyncAPI spec this module provides assistance. The suggested way to use this module is: A template file defining the general info rmation, channels and components.messages using the AsyncAPI spec. components.schemas should be omitted. The schema is defined and documented as Java classes in the code as they are used in message handlers and consumers. Jackson, Jakarta Validation and Swagger 2 annotations can be used for documentation. The root classes of messages are referenced in components.messages.YourMessage.payload.$ref as class://your.package.MessageModel . The AsyncApiGenerator is used to combine the template and the generated Json Schema of the models to a self-contained spec file. The generated AsyncAPI spec is committed into source control. This way, the commit history will show intended and unintended changes to the API and the API spec is accessible any time without executing any code. The API can be view in AsyncAPI Studio . It is suggested to use it as a test dependency, build the AsyncAPI in a unit test and verify that it is up-to-date. The GoldenFileAssertions from the test module help here. Example: Build AsyncAPI for Cars asyncapi_template.yaml CarManufactured \u2026 AsyncApiTest Generated asyncapi.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 asyncapi : '2.5.0' id : 'urn:org:sdase:example:cars' defaultContentType : application/json info : title : Cars Example description : This example demonstrates how to define events around *cars*. version : '1.0.0' channels : 'car-events' : publish : operationId : publishCarEvents summary : Car related events description : These are all events that are related to a car message : oneOf : - $ref : '#/components/messages/CarManufactured' - $ref : '#/components/messages/CarScrapped' components : messages : CarManufactured : title : Car Manufactured description : An event that represents when a new car is manufactured payload : # referencing the full name of the Class $ref : 'class://org.sdase.commons.spring.boot.asyncapi.test.data.models.CarManufactured' CarScrapped : title : Car Scrapped description : An event that represents when a car is scrapped payload : # referencing the full name of the Class $ref : 'class://org.sdase.commons.spring.boot.asyncapi.test.data.models.CarScrapped' 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.asyncapi.test.data.models ; import com.fasterxml.jackson.annotation.JsonPropertyDescription ; import io.swagger.v3.oas.annotations.media.Schema ; import jakarta.validation.constraints.NotBlank ; import jakarta.validation.constraints.NotNull ; import java.time.Instant ; @Schema ( title = \"Car manufactured\" , description = \"A new car was manufactured\" ) @SuppressWarnings ( \"unused\" ) public class CarManufactured extends BaseEvent { @NotBlank @Schema ( description = \"The registration of the vehicle\" , example = \"BB324A81\" ) private String vehicleRegistration ; @NotNull @JsonPropertyDescription ( \"The time of manufacturing\" ) private Instant date ; @NotNull @JsonPropertyDescription ( \"The model of the car\" ) private CarModel model ; public String getVehicleRegistration () { return vehicleRegistration ; } public CarManufactured setVehicleRegistration ( String vehicleRegistration ) { this . vehicleRegistration = vehicleRegistration ; return this ; } public Instant getDate () { return date ; } public CarManufactured setDate ( Instant date ) { this . date = date ; return this ; } public CarModel getModel () { return model ; } public CarManufactured setModel ( CarModel model ) { this . model = model ; return this ; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.asyncapi.test.data.models ; import com.fasterxml.jackson.annotation.JsonClassDescription ; import com.fasterxml.jackson.annotation.JsonPropertyDescription ; import io.swagger.v3.oas.annotations.media.Schema ; import jakarta.validation.constraints.NotBlank ; import jakarta.validation.constraints.NotNull ; import java.time.Instant ; @JsonClassDescription ( \"A car was scrapped\" ) @SuppressWarnings ( \"unused\" ) public class CarScrapped extends BaseEvent { @NotBlank @JsonPropertyDescription ( \"The registration of the vehicle\" ) @Schema ( example = \"BB324A81\" ) private String vehicleRegistration ; @NotNull @JsonPropertyDescription ( \"The time of scrapping\" ) private Instant date ; @JsonPropertyDescription ( \"The location where the car was scrapped\" ) private String location ; public String getVehicleRegistration () { return vehicleRegistration ; } public CarScrapped setVehicleRegistration ( String vehicleRegistration ) { this . vehicleRegistration = vehicleRegistration ; return this ; } public Instant getDate () { return date ; } public CarScrapped setDate ( Instant date ) { this . date = date ; return this ; } public String getLocation () { return location ; } public CarScrapped setLocation ( String location ) { this . location = location ; return this ; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.asyncapi ; import java.io.IOException ; import java.nio.file.Path ; import java.nio.file.Paths ; import org.junit.jupiter.api.Test ; import org.sdase.commons.spring.boot.web.testing.GoldenFileAssertions ; class AsyncApiTest { @Test void generateAndVerifySpec () throws IOException { // get template var template = getClass (). getResource ( \"/demo/asyncapi_template.yaml\" ); // generate AsyncAPI yaml String expected = AsyncApiGenerator . builder (). withAsyncApiBase ( template ). generateYaml (); // specify where to store the result, e.g. Path.of(\"asyncapi.yaml\") for the project root. Path filePath = Paths . get ( \"asyncapi.yaml\" ); // check and update the file GoldenFileAssertions . assertThat ( filePath ). hasYamlContentAndUpdateGolden ( expected ); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 --- asyncapi : \"2.5.0\" id : \"urn:org:sdase:example:cars\" defaultContentType : \"application/json\" info : title : \"Cars Example\" description : \"This example demonstrates how to define events around *cars*.\" version : \"1.0.0\" channels : car-events : publish : operationId : \"publishCarEvents\" summary : \"Car related events\" description : \"These are all events that are related to a car\" message : oneOf : - $ref : \"#/components/messages/CarManufactured\" - $ref : \"#/components/messages/CarScrapped\" components : messages : CarManufactured : title : \"Car Manufactured\" description : \"An event that represents when a new car is manufactured\" payload : $ref : \"#/components/schemas/CarManufactured\" CarScrapped : title : \"Car Scrapped\" description : \"An event that represents when a car is scrapped\" payload : $ref : \"#/components/schemas/CarScrapped\" schemas : CarManufactured : allOf : - type : \"object\" properties : id : type : \"string\" description : \"The id of the message\" minLength : 1 examples : - \"626A0F21-D940-4B44-BD36-23F0F567B0D0\" type : allOf : - $ref : \"#/components/schemas/Type\" - description : \"The type of message\" vehicleRegistration : type : \"string\" description : \"The registration of the vehicle\" minLength : 1 examples : - \"BB324A81\" date : type : \"string\" format : \"date-time\" description : \"The time of manufacturing\" model : allOf : - $ref : \"#/components/schemas/CarModel\" - description : \"The model of the car\" required : - \"id\" - \"vehicleRegistration\" - \"date\" - \"model\" title : \"Car manufactured\" description : \"A new car was manufactured\" - type : \"object\" properties : type : const : \"CAR_MANUFACTURED\" required : - \"type\" CarModel : anyOf : - $ref : \"#/components/schemas/Electrical\" - $ref : \"#/components/schemas/Combustion\" CarScrapped : allOf : - type : \"object\" properties : id : type : \"string\" description : \"The id of the message\" minLength : 1 examples : - \"626A0F21-D940-4B44-BD36-23F0F567B0D0\" type : allOf : - $ref : \"#/components/schemas/Type\" - description : \"The type of message\" vehicleRegistration : type : \"string\" description : \"The registration of the vehicle\" minLength : 1 examples : - \"BB324A81\" date : type : \"string\" format : \"date-time\" description : \"The time of scrapping\" location : type : \"string\" description : \"The location where the car was scrapped\" required : - \"id\" - \"vehicleRegistration\" - \"date\" description : \"A car was scrapped\" - type : \"object\" properties : type : const : \"CAR_SCRAPPED\" required : - \"type\" Combustion : allOf : - type : \"object\" properties : name : type : \"string\" description : \"The name of the car model\" examples : - \"Tesla Roadster\" engineType : type : \"string\" description : \"The type of engine\" tankVolume : type : \"integer\" description : \"The capacity of the tank in liter\" examples : - 95 required : - \"tankVolume\" title : \"Combustion engine\" description : \"An car model with a combustion engine\" - type : \"object\" properties : engineType : const : \"COMBUSTION\" required : - \"engineType\" Electrical : allOf : - type : \"object\" properties : name : type : \"string\" description : \"The name of the car model\" examples : - \"Tesla Roadster\" engineType : type : \"string\" description : \"The type of engine\" batteryCapacity : type : \"integer\" description : \"The capacity of the battery in kwH\" examples : - 200 required : - \"batteryCapacity\" title : \"Electrical engine\" description : \"An car model with an electrical engine\" - type : \"object\" properties : engineType : const : \"ELECTRICAL\" required : - \"engineType\" Type : type : \"string\" enum : - \"CAR_MANUFACTURED\" - \"CAR_SCRAPPED\" Usage with Existing Schemas \u00b6 In some cases it is not possible to generate a schema with appropriate documentation, e.g. when a framework requires to use classes from dependencies that do not contain the expected annotations. In this case the schema may be added to the template. This should be used as fallback only, because the schema is not connected to the actual code, it may diverge over time. Example: Build AsyncAPI with handcrafted schema template_with_schema.yaml Created ApiWithSchemaTest Generated asyncapi-schema.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 asyncapi : '2.5.0' id : 'urn:org:sdase:example' defaultContentType : application/json info : title : Example description : This example demonstrates how to define messages with hand crafted schemas. version : '1.0.0' channels : 'car-events' : publish : summary : An entity stream description : What happens to an entity message : oneOf : - $ref : '#/components/messages/Created' - $ref : '#/components/messages/Deleted' components : messages : Created : title : Entity created payload : # referencing the full name of the Class $ref : 'class://org.sdase.commons.spring.boot.asyncapi.test.data.models.Created' Deleted : title : Entity deleted description : Deletion of the entity is represented by an external tombstone message. payload : # referencing the existing schema $ref : '#/components/schemas/Tombstone' schemas : Tombstone : type : object description : | The tombstone event is published to indicate that the entity has been deleted. All copies of data related to the entity must be deleted. properties : id : type : string tombstone : type : boolean const : true 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.asyncapi.test.data.models ; import jakarta.validation.constraints.NotBlank ; import jakarta.validation.constraints.NotNull ; import jakarta.validation.constraints.Pattern ; @SuppressWarnings ( \"unused\" ) public class Created { @NotNull @Pattern ( regexp = \"[a-zA-Z0-9-_]{10,}\" ) private String id ; @NotBlank private String name ; public String getId () { return id ; } public Created setId ( String id ) { this . id = id ; return this ; } public String getName () { return name ; } public Created setName ( String name ) { this . name = name ; return this ; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.asyncapi ; import java.io.IOException ; import java.nio.file.Path ; import java.nio.file.Paths ; import org.junit.jupiter.api.Test ; import org.sdase.commons.spring.boot.web.testing.GoldenFileAssertions ; class ApiWithSchemaTest { @Test void generateAndVerifySpec () throws IOException { // get template var template = getClass (). getResource ( \"/demo/template_with_schema.yaml\" ); // generate AsyncAPI yaml String expected = AsyncApiGenerator . builder (). withAsyncApiBase ( template ). generateYaml (); // specify where to store the result, e.g. Path.of(\"asyncapi.yaml\") for the project root. Path filePath = Paths . get ( \"asyncapi-schema.yaml\" ); // check and update the file GoldenFileAssertions . assertThat ( filePath ). hasYamlContentAndUpdateGolden ( expected ); } } 1 Generating Schema Files \u00b6 If desired, the module also allows to generate the JSON schema files, for example to use them to validate test data. Please take a look at JsonSchemaBuilder and it's implementations.","title":"AsyncAPI"},{"location":"asyncapi/#sda-commons-async-api","text":"Experimental Please be aware that SDA SE is likely to change or remove this artifact in the future This module contains the AsyncApiGenerator to generate AsyncAPI specs from a template and model classes in a code first approach. The AsyncAPI specification is the industry standard for defining asynchronous APIs.","title":"sda-commons-async-api"},{"location":"asyncapi/#usage","text":"If the code first approach is used to create an AsyncAPI spec this module provides assistance. The suggested way to use this module is: A template file defining the general info rmation, channels and components.messages using the AsyncAPI spec. components.schemas should be omitted. The schema is defined and documented as Java classes in the code as they are used in message handlers and consumers. Jackson, Jakarta Validation and Swagger 2 annotations can be used for documentation. The root classes of messages are referenced in components.messages.YourMessage.payload.$ref as class://your.package.MessageModel . The AsyncApiGenerator is used to combine the template and the generated Json Schema of the models to a self-contained spec file. The generated AsyncAPI spec is committed into source control. This way, the commit history will show intended and unintended changes to the API and the API spec is accessible any time without executing any code. The API can be view in AsyncAPI Studio . It is suggested to use it as a test dependency, build the AsyncAPI in a unit test and verify that it is up-to-date. The GoldenFileAssertions from the test module help here. Example: Build AsyncAPI for Cars asyncapi_template.yaml CarManufactured \u2026 AsyncApiTest Generated asyncapi.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 asyncapi : '2.5.0' id : 'urn:org:sdase:example:cars' defaultContentType : application/json info : title : Cars Example description : This example demonstrates how to define events around *cars*. version : '1.0.0' channels : 'car-events' : publish : operationId : publishCarEvents summary : Car related events description : These are all events that are related to a car message : oneOf : - $ref : '#/components/messages/CarManufactured' - $ref : '#/components/messages/CarScrapped' components : messages : CarManufactured : title : Car Manufactured description : An event that represents when a new car is manufactured payload : # referencing the full name of the Class $ref : 'class://org.sdase.commons.spring.boot.asyncapi.test.data.models.CarManufactured' CarScrapped : title : Car Scrapped description : An event that represents when a car is scrapped payload : # referencing the full name of the Class $ref : 'class://org.sdase.commons.spring.boot.asyncapi.test.data.models.CarScrapped' 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.asyncapi.test.data.models ; import com.fasterxml.jackson.annotation.JsonPropertyDescription ; import io.swagger.v3.oas.annotations.media.Schema ; import jakarta.validation.constraints.NotBlank ; import jakarta.validation.constraints.NotNull ; import java.time.Instant ; @Schema ( title = \"Car manufactured\" , description = \"A new car was manufactured\" ) @SuppressWarnings ( \"unused\" ) public class CarManufactured extends BaseEvent { @NotBlank @Schema ( description = \"The registration of the vehicle\" , example = \"BB324A81\" ) private String vehicleRegistration ; @NotNull @JsonPropertyDescription ( \"The time of manufacturing\" ) private Instant date ; @NotNull @JsonPropertyDescription ( \"The model of the car\" ) private CarModel model ; public String getVehicleRegistration () { return vehicleRegistration ; } public CarManufactured setVehicleRegistration ( String vehicleRegistration ) { this . vehicleRegistration = vehicleRegistration ; return this ; } public Instant getDate () { return date ; } public CarManufactured setDate ( Instant date ) { this . date = date ; return this ; } public CarModel getModel () { return model ; } public CarManufactured setModel ( CarModel model ) { this . model = model ; return this ; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.asyncapi.test.data.models ; import com.fasterxml.jackson.annotation.JsonClassDescription ; import com.fasterxml.jackson.annotation.JsonPropertyDescription ; import io.swagger.v3.oas.annotations.media.Schema ; import jakarta.validation.constraints.NotBlank ; import jakarta.validation.constraints.NotNull ; import java.time.Instant ; @JsonClassDescription ( \"A car was scrapped\" ) @SuppressWarnings ( \"unused\" ) public class CarScrapped extends BaseEvent { @NotBlank @JsonPropertyDescription ( \"The registration of the vehicle\" ) @Schema ( example = \"BB324A81\" ) private String vehicleRegistration ; @NotNull @JsonPropertyDescription ( \"The time of scrapping\" ) private Instant date ; @JsonPropertyDescription ( \"The location where the car was scrapped\" ) private String location ; public String getVehicleRegistration () { return vehicleRegistration ; } public CarScrapped setVehicleRegistration ( String vehicleRegistration ) { this . vehicleRegistration = vehicleRegistration ; return this ; } public Instant getDate () { return date ; } public CarScrapped setDate ( Instant date ) { this . date = date ; return this ; } public String getLocation () { return location ; } public CarScrapped setLocation ( String location ) { this . location = location ; return this ; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.asyncapi ; import java.io.IOException ; import java.nio.file.Path ; import java.nio.file.Paths ; import org.junit.jupiter.api.Test ; import org.sdase.commons.spring.boot.web.testing.GoldenFileAssertions ; class AsyncApiTest { @Test void generateAndVerifySpec () throws IOException { // get template var template = getClass (). getResource ( \"/demo/asyncapi_template.yaml\" ); // generate AsyncAPI yaml String expected = AsyncApiGenerator . builder (). withAsyncApiBase ( template ). generateYaml (); // specify where to store the result, e.g. Path.of(\"asyncapi.yaml\") for the project root. Path filePath = Paths . get ( \"asyncapi.yaml\" ); // check and update the file GoldenFileAssertions . assertThat ( filePath ). hasYamlContentAndUpdateGolden ( expected ); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 --- asyncapi : \"2.5.0\" id : \"urn:org:sdase:example:cars\" defaultContentType : \"application/json\" info : title : \"Cars Example\" description : \"This example demonstrates how to define events around *cars*.\" version : \"1.0.0\" channels : car-events : publish : operationId : \"publishCarEvents\" summary : \"Car related events\" description : \"These are all events that are related to a car\" message : oneOf : - $ref : \"#/components/messages/CarManufactured\" - $ref : \"#/components/messages/CarScrapped\" components : messages : CarManufactured : title : \"Car Manufactured\" description : \"An event that represents when a new car is manufactured\" payload : $ref : \"#/components/schemas/CarManufactured\" CarScrapped : title : \"Car Scrapped\" description : \"An event that represents when a car is scrapped\" payload : $ref : \"#/components/schemas/CarScrapped\" schemas : CarManufactured : allOf : - type : \"object\" properties : id : type : \"string\" description : \"The id of the message\" minLength : 1 examples : - \"626A0F21-D940-4B44-BD36-23F0F567B0D0\" type : allOf : - $ref : \"#/components/schemas/Type\" - description : \"The type of message\" vehicleRegistration : type : \"string\" description : \"The registration of the vehicle\" minLength : 1 examples : - \"BB324A81\" date : type : \"string\" format : \"date-time\" description : \"The time of manufacturing\" model : allOf : - $ref : \"#/components/schemas/CarModel\" - description : \"The model of the car\" required : - \"id\" - \"vehicleRegistration\" - \"date\" - \"model\" title : \"Car manufactured\" description : \"A new car was manufactured\" - type : \"object\" properties : type : const : \"CAR_MANUFACTURED\" required : - \"type\" CarModel : anyOf : - $ref : \"#/components/schemas/Electrical\" - $ref : \"#/components/schemas/Combustion\" CarScrapped : allOf : - type : \"object\" properties : id : type : \"string\" description : \"The id of the message\" minLength : 1 examples : - \"626A0F21-D940-4B44-BD36-23F0F567B0D0\" type : allOf : - $ref : \"#/components/schemas/Type\" - description : \"The type of message\" vehicleRegistration : type : \"string\" description : \"The registration of the vehicle\" minLength : 1 examples : - \"BB324A81\" date : type : \"string\" format : \"date-time\" description : \"The time of scrapping\" location : type : \"string\" description : \"The location where the car was scrapped\" required : - \"id\" - \"vehicleRegistration\" - \"date\" description : \"A car was scrapped\" - type : \"object\" properties : type : const : \"CAR_SCRAPPED\" required : - \"type\" Combustion : allOf : - type : \"object\" properties : name : type : \"string\" description : \"The name of the car model\" examples : - \"Tesla Roadster\" engineType : type : \"string\" description : \"The type of engine\" tankVolume : type : \"integer\" description : \"The capacity of the tank in liter\" examples : - 95 required : - \"tankVolume\" title : \"Combustion engine\" description : \"An car model with a combustion engine\" - type : \"object\" properties : engineType : const : \"COMBUSTION\" required : - \"engineType\" Electrical : allOf : - type : \"object\" properties : name : type : \"string\" description : \"The name of the car model\" examples : - \"Tesla Roadster\" engineType : type : \"string\" description : \"The type of engine\" batteryCapacity : type : \"integer\" description : \"The capacity of the battery in kwH\" examples : - 200 required : - \"batteryCapacity\" title : \"Electrical engine\" description : \"An car model with an electrical engine\" - type : \"object\" properties : engineType : const : \"ELECTRICAL\" required : - \"engineType\" Type : type : \"string\" enum : - \"CAR_MANUFACTURED\" - \"CAR_SCRAPPED\"","title":"Usage"},{"location":"asyncapi/#usage-with-existing-schemas","text":"In some cases it is not possible to generate a schema with appropriate documentation, e.g. when a framework requires to use classes from dependencies that do not contain the expected annotations. In this case the schema may be added to the template. This should be used as fallback only, because the schema is not connected to the actual code, it may diverge over time. Example: Build AsyncAPI with handcrafted schema template_with_schema.yaml Created ApiWithSchemaTest Generated asyncapi-schema.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 asyncapi : '2.5.0' id : 'urn:org:sdase:example' defaultContentType : application/json info : title : Example description : This example demonstrates how to define messages with hand crafted schemas. version : '1.0.0' channels : 'car-events' : publish : summary : An entity stream description : What happens to an entity message : oneOf : - $ref : '#/components/messages/Created' - $ref : '#/components/messages/Deleted' components : messages : Created : title : Entity created payload : # referencing the full name of the Class $ref : 'class://org.sdase.commons.spring.boot.asyncapi.test.data.models.Created' Deleted : title : Entity deleted description : Deletion of the entity is represented by an external tombstone message. payload : # referencing the existing schema $ref : '#/components/schemas/Tombstone' schemas : Tombstone : type : object description : | The tombstone event is published to indicate that the entity has been deleted. All copies of data related to the entity must be deleted. properties : id : type : string tombstone : type : boolean const : true 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.asyncapi.test.data.models ; import jakarta.validation.constraints.NotBlank ; import jakarta.validation.constraints.NotNull ; import jakarta.validation.constraints.Pattern ; @SuppressWarnings ( \"unused\" ) public class Created { @NotNull @Pattern ( regexp = \"[a-zA-Z0-9-_]{10,}\" ) private String id ; @NotBlank private String name ; public String getId () { return id ; } public Created setId ( String id ) { this . id = id ; return this ; } public String getName () { return name ; } public Created setName ( String name ) { this . name = name ; return this ; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.asyncapi ; import java.io.IOException ; import java.nio.file.Path ; import java.nio.file.Paths ; import org.junit.jupiter.api.Test ; import org.sdase.commons.spring.boot.web.testing.GoldenFileAssertions ; class ApiWithSchemaTest { @Test void generateAndVerifySpec () throws IOException { // get template var template = getClass (). getResource ( \"/demo/template_with_schema.yaml\" ); // generate AsyncAPI yaml String expected = AsyncApiGenerator . builder (). withAsyncApiBase ( template ). generateYaml (); // specify where to store the result, e.g. Path.of(\"asyncapi.yaml\") for the project root. Path filePath = Paths . get ( \"asyncapi-schema.yaml\" ); // check and update the file GoldenFileAssertions . assertThat ( filePath ). hasYamlContentAndUpdateGolden ( expected ); } } 1","title":"Usage with Existing Schemas"},{"location":"asyncapi/#generating-schema-files","text":"If desired, the module also allows to generate the JSON schema files, for example to use them to validate test data. Please take a look at JsonSchemaBuilder and it's implementations.","title":"Generating Schema Files"},{"location":"cloudevents/","text":"CloudEvents \u00b6 The module sda-commons-cloudevents provides some glue code to work with CloudEvents on top of Apache Kafka. Introduction \u00b6 CloudEvents is a general standard that can be used in combination with your favourite eventing tool like ActiveMQ or Kafka. The CloudEvents specification defines concrete bindings to define how the general specification should be applied to a specific tool. This module provides POJOs to use CloudEvent's structured content mode . Producing CloudEvents \u00b6 For simplicity, we recommend to extend our base class and add your own class for the data property. Additional documentation can be added at class level and for the data type. Custom event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.cloudevents.app.partner ; import com.fasterxml.jackson.annotation.JsonClassDescription ; import io.swagger.v3.oas.annotations.media.Schema ; import java.net.URI ; import org.sdase.commons.spring.boot.cloudevents.CloudEventV1 ; @JsonClassDescription ( \"An event that is published when a partner has been created.\" ) public class PartnerCreatedEvent extends CloudEventV1 < PartnerCreatedEvent . PartnerCreated > { private static final String DEFAULT_SOURCE = \"/SDA-SE/partner/partner-stack/partner-service\" ; private static final String DEFAULT_TYPE = \"com.sdase.partner.ods.partner.created\" ; public PartnerCreatedEvent () { super (); super . setSource ( URI . create ( DEFAULT_SOURCE )); super . setType ( DEFAULT_TYPE ); } @JsonClassDescription ( \"Details about the created partner.\" ) public record PartnerCreated ( @Schema ( description = \"The unique id of a partner.\" , example = \"FF427BC8-B38F-43CC-8AAB-512843808A18\" ) String id ) {} } You can use a standard org.springframework.kafka.support.serializer.JsonSerializer to publish the event. Consuming CloudEvents \u00b6 You can use org.springframework.kafka.support.serializer.JsonDeserializer to consume CloudEvents. Polymorphism \u00b6 Usually, an eventing API consists of multiple events related to one aggregate. You can use Jacksons subtype features to define multiple events in one model. Multiple events in one API 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.cloudevents.app.polymorphism ; import static com.fasterxml.jackson.annotation.JsonTypeInfo.As.EXISTING_PROPERTY ; import static com.fasterxml.jackson.annotation.JsonTypeInfo.Id.NAME ; import static org.sdase.commons.spring.boot.cloudevents.app.polymorphism.CarLifecycleEvents.MANUFACTURED ; import static org.sdase.commons.spring.boot.cloudevents.app.polymorphism.CarLifecycleEvents.SCRAPPED ; import com.fasterxml.jackson.annotation.JsonSubTypes ; import com.fasterxml.jackson.annotation.JsonSubTypes.Type ; import com.fasterxml.jackson.annotation.JsonTypeInfo ; import org.sdase.commons.spring.boot.cloudevents.CloudEventV1 ; @JsonTypeInfo ( use = NAME , property = \"type\" , visible = true , include = EXISTING_PROPERTY ) @JsonSubTypes ({ @Type ( value = CarLifecycleEvents . CarManufactured . class , name = MANUFACTURED ), @Type ( value = CarLifecycleEvents . CarScrapped . class , name = SCRAPPED ) }) public abstract class CarLifecycleEvents < T > extends CloudEventV1 < T > { static final String MANUFACTURED = \"se.sda.car.manufactured\" ; static final String SCRAPPED = \"se.sda.car.scrapped\" ; public static class CarManufactured extends CarLifecycleEvents < CarManufactured . CarManufacturedData > { public CarManufactured () { super (); super . setType ( MANUFACTURED ); } public record CarManufacturedData ( String brand , String model ) {} } public static class CarScrapped extends CarLifecycleEvents < CarScrapped . CarScrappedData > { public CarScrapped () { super (); super . setType ( SCRAPPED ); } public record CarScrappedData ( ScrapReason reason ) { public enum ScrapReason { ACCIDENT , TECHNICAL_DAMAGE } } } } Note: If such a model grows, you may want to extract the data types as top level classes.","title":"Cloud Events"},{"location":"cloudevents/#cloudevents","text":"The module sda-commons-cloudevents provides some glue code to work with CloudEvents on top of Apache Kafka.","title":"CloudEvents"},{"location":"cloudevents/#introduction","text":"CloudEvents is a general standard that can be used in combination with your favourite eventing tool like ActiveMQ or Kafka. The CloudEvents specification defines concrete bindings to define how the general specification should be applied to a specific tool. This module provides POJOs to use CloudEvent's structured content mode .","title":"Introduction"},{"location":"cloudevents/#producing-cloudevents","text":"For simplicity, we recommend to extend our base class and add your own class for the data property. Additional documentation can be added at class level and for the data type. Custom event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.cloudevents.app.partner ; import com.fasterxml.jackson.annotation.JsonClassDescription ; import io.swagger.v3.oas.annotations.media.Schema ; import java.net.URI ; import org.sdase.commons.spring.boot.cloudevents.CloudEventV1 ; @JsonClassDescription ( \"An event that is published when a partner has been created.\" ) public class PartnerCreatedEvent extends CloudEventV1 < PartnerCreatedEvent . PartnerCreated > { private static final String DEFAULT_SOURCE = \"/SDA-SE/partner/partner-stack/partner-service\" ; private static final String DEFAULT_TYPE = \"com.sdase.partner.ods.partner.created\" ; public PartnerCreatedEvent () { super (); super . setSource ( URI . create ( DEFAULT_SOURCE )); super . setType ( DEFAULT_TYPE ); } @JsonClassDescription ( \"Details about the created partner.\" ) public record PartnerCreated ( @Schema ( description = \"The unique id of a partner.\" , example = \"FF427BC8-B38F-43CC-8AAB-512843808A18\" ) String id ) {} } You can use a standard org.springframework.kafka.support.serializer.JsonSerializer to publish the event.","title":"Producing CloudEvents"},{"location":"cloudevents/#consuming-cloudevents","text":"You can use org.springframework.kafka.support.serializer.JsonDeserializer to consume CloudEvents.","title":"Consuming CloudEvents"},{"location":"cloudevents/#polymorphism","text":"Usually, an eventing API consists of multiple events related to one aggregate. You can use Jacksons subtype features to define multiple events in one model. Multiple events in one API 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 /* * Copyright 2022- SDA SE Open Industry Solutions (https://www.sda.se) * * Use of this source code is governed by an MIT-style * license that can be found in the LICENSE file or at * https://opensource.org/licenses/MIT. */ package org.sdase.commons.spring.boot.cloudevents.app.polymorphism ; import static com.fasterxml.jackson.annotation.JsonTypeInfo.As.EXISTING_PROPERTY ; import static com.fasterxml.jackson.annotation.JsonTypeInfo.Id.NAME ; import static org.sdase.commons.spring.boot.cloudevents.app.polymorphism.CarLifecycleEvents.MANUFACTURED ; import static org.sdase.commons.spring.boot.cloudevents.app.polymorphism.CarLifecycleEvents.SCRAPPED ; import com.fasterxml.jackson.annotation.JsonSubTypes ; import com.fasterxml.jackson.annotation.JsonSubTypes.Type ; import com.fasterxml.jackson.annotation.JsonTypeInfo ; import org.sdase.commons.spring.boot.cloudevents.CloudEventV1 ; @JsonTypeInfo ( use = NAME , property = \"type\" , visible = true , include = EXISTING_PROPERTY ) @JsonSubTypes ({ @Type ( value = CarLifecycleEvents . CarManufactured . class , name = MANUFACTURED ), @Type ( value = CarLifecycleEvents . CarScrapped . class , name = SCRAPPED ) }) public abstract class CarLifecycleEvents < T > extends CloudEventV1 < T > { static final String MANUFACTURED = \"se.sda.car.manufactured\" ; static final String SCRAPPED = \"se.sda.car.scrapped\" ; public static class CarManufactured extends CarLifecycleEvents < CarManufactured . CarManufacturedData > { public CarManufactured () { super (); super . setType ( MANUFACTURED ); } public record CarManufacturedData ( String brand , String model ) {} } public static class CarScrapped extends CarLifecycleEvents < CarScrapped . CarScrappedData > { public CarScrapped () { super (); super . setType ( SCRAPPED ); } public record CarScrappedData ( ScrapReason reason ) { public enum ScrapReason { ACCIDENT , TECHNICAL_DAMAGE } } } } Note: If such a model grows, you may want to extract the data types as top level classes.","title":"Polymorphism"},{"location":"metadata-context/","text":"The Metadata Context \u00b6 Purpose \u00b6 The metadata context keeps contextual information of a business process across all involved microservices. The information that is stored in the metadata context is solely defined by the environment. Defining a metadata context for an environment is optional. If no context is defined, no metadata will be available for any service. The metadata should not affect the core business logic of a service but can be used to affect how a service communicates with the user (e.g. communication methods, templates or domains used for links) or how statistical data is evaluated. As the metadata is tracked for a whole business process across all involved services, it supports that services which are not in a synchronous call chain, can behave differently based on the entry point. All services in between and the service that uses information from the metadata context don't need to clutter their API with non-functional information, that may be different in each environment, to support features a service in the end of a process provides based on the initial user request. A service must be able to fulfill its purpose without any information from the metadata context. A service may use information from the metadata context to use or apply different business related or non-functional configuration for a specific business process but must have a default for these configurations as well. The metadata context is technically transferred via HTTP headers and Kafka message headers between services in the same environment. At runtime, it is held in a thread local variable to be available for the current process. The example below explains what kind of problems the metadata context can solve. Configuration and structure \u00b6 A metadata context consists of well-defined fields. They must be defined equally for all services in the same environment. Services based on this library use the environment variable METADATA_FIELDS as comma separated list of field names automatically. Field names are used as HTTP and Kafka message header keys. The fields must not conflict with officially defined header keys. The data type of the value is always text, represented as String in this library. Multiple values are supported for each field in a specific context. To simplify parsing, according to RFC 9110 5.2 , values must not contain commas. Creating metadata context information \u00b6 A metadata context for a specific business process should be initialized as early as possible. Technically the context is created by passing the headers of defined metadata fields to a service that supports the metadata context. No standard component should create a metadata context initially, because the metadata context always depends on the environment. The easiest way to initially create a metadata context is to add the respective headers in a proxy that exposes a service, e.g. an Ingress in Kubernetes. The proxy may derive the metadata information from the used domain, query params, user agent headers and other information available in the request. If getting the metadata information is more complex, an environment specific interaction service (according to the SDA service architecture) may be implemented and create the context programmatically. This service should act like a proxy and forward the request to a standard business service. Services that recreate a context for a process that was interrupted, use the context they stored along with the entity to activate it for current thread. Support metadata context in a service \u00b6 Any service that communicates synchronously with the HTTP platform clients provided by this library and asynchronously with Kafka consumers and producers initialized by this library and has no persistence layer most likely supports the metadata context. Exceptions may apply if business processes are interrupted or asynchronously processed without saving business data in a database. Services that persist data and therefore interrupt the business process until the data is loaded into memory again can support the metadata context if they store it along with the entity and recreate the runtime context when the entity is loaded and processed. Service that interrupts a process in memory or proceeds asynchronously must transfer the metadata context to the new thread to support the metadata context. Services that fulfill these requirements should mention the support of the metadata context in their deployment documentation. They should also mention the configuration environment variable METADATA_FIELDS that is automatically available for all services based on this library. Services that read information from the metadata context shall not use hard coded field names. They must not expect that specific fields are available in an environment. The environment, technically the operations team, is in the lead to define which metadata is available. A service only knows a specific purpose for which it supports metadata information. Whenever a service supports decisions based on metadata configuration, it must provide a configuration option which naming is based on the purpose to let the operations team define which metadata field should be used. The library supports to derive the actual field names from environment variables. The values are dependent on the environment as well. Therefore, the service must provide configuration options to map values of the metadata field to values it understands. Supporting information from the metadata context to distinguish the behavior of the service can be very complex due to the loose coupling and the high flexibility for the owners of the environment. Every configuration that can be modified by information of the metadata context must have a default as well, because the service may be used in an environment without configured metadata context or in a specific business process is no context available. How a service behaves and is configured when metadata context is available must be described in the deployment documentation to make this complex flexibility manageable by the operations team. Example: Template selection \u00b6 Imagine a business case that is advertised on different landing pages, where the user enters information and receives a result asynchronously via email. The email sent to the user should match the style of the landing page. Each landing page is available on a dedicated domain but all lead to the same backend services. The services may communicate asynchronously or even need additional information from the backoffice team to create the result. The operations team defines the METADATA_FIELDS as landing-page-source for all services in the environment to identify the landing page where the user entered their data. They configure the Ingress, to set either christmas-special or summer-edition as landing-page-source header - based on the domain of two landing pages. No service but the one that is generating the email is affected by this information. The core business rules that determine the result only need the information the user entered. The email content can be created independently as well. Only the service that renders the HTML part of the email must be aware of the source of the request. Therefore, it must have the ability to use multiple templates. Besides other optional input to determine the template to use, it must be configurable to use information from the metadata context. To keep the flexibility the metadata context offers, it defines the environment variable TEMPLATE_HINT_METADATA_FIELD . The operations team configures it as landing-page-source . The service developer can use TEMPLATE_HINT_METADATA_FIELD in the MetadataContext API to get the values of current context, but must be aware that the result is empty or contains multiple values. Additionally, the email templating service must provide configuration options to select a template based on the value as well as providing the templates. The templates and the configuration could be mounted as files in the container. The latter may be a yaml file and look like this: 1 2 3 4 5 6 7 # templates.yaml default : /mounts/templates/default.hbs byMetadata : - metadataValue : christmas-special templateFile : /mounts/templates/xmas.hbs - metadataValue : summer-edition templateFile : /mounts/templates/summer.hbs Variant of template selection \u00b6 The email example is very flexible and would support other environments as well, e.g. when A/B testing is used to find the perfect color for the use case. The email service is not involved in the evaluation of the A/B test, but has to send emails that match the styles of the A/B test. Such user tests will most likely be evaluated by dedicated UX tools and not by the backend services in the environment. No business service should be affected by this information. However, a service creating statistics for the use case could support the metadata context as well and provide additional evaluation for each variant. The frontend must provide any information in the request to distinguish between A and B. Then the proxy can convert the information in a header that is sent to the backend service, e.g. ab-variant=neon . The operations team of this environment will configure METADATA_FIELDS for all services and TEMPLATE_HINT_METADATA_FIELD as ab-variant and the email templating like this: 1 2 3 4 5 6 7 # templates.yaml default : /mounts/templates/default.hbs byMetadata : - metadataValue : pastel templateFile : /mounts/templates/pastel.hbs - metadataValue : neon templateFile : /mounts/templates/neon.hbs","title":"Metadata Context"},{"location":"metadata-context/#the-metadata-context","text":"","title":"The Metadata Context"},{"location":"metadata-context/#purpose","text":"The metadata context keeps contextual information of a business process across all involved microservices. The information that is stored in the metadata context is solely defined by the environment. Defining a metadata context for an environment is optional. If no context is defined, no metadata will be available for any service. The metadata should not affect the core business logic of a service but can be used to affect how a service communicates with the user (e.g. communication methods, templates or domains used for links) or how statistical data is evaluated. As the metadata is tracked for a whole business process across all involved services, it supports that services which are not in a synchronous call chain, can behave differently based on the entry point. All services in between and the service that uses information from the metadata context don't need to clutter their API with non-functional information, that may be different in each environment, to support features a service in the end of a process provides based on the initial user request. A service must be able to fulfill its purpose without any information from the metadata context. A service may use information from the metadata context to use or apply different business related or non-functional configuration for a specific business process but must have a default for these configurations as well. The metadata context is technically transferred via HTTP headers and Kafka message headers between services in the same environment. At runtime, it is held in a thread local variable to be available for the current process. The example below explains what kind of problems the metadata context can solve.","title":"Purpose"},{"location":"metadata-context/#configuration-and-structure","text":"A metadata context consists of well-defined fields. They must be defined equally for all services in the same environment. Services based on this library use the environment variable METADATA_FIELDS as comma separated list of field names automatically. Field names are used as HTTP and Kafka message header keys. The fields must not conflict with officially defined header keys. The data type of the value is always text, represented as String in this library. Multiple values are supported for each field in a specific context. To simplify parsing, according to RFC 9110 5.2 , values must not contain commas.","title":"Configuration and structure"},{"location":"metadata-context/#creating-metadata-context-information","text":"A metadata context for a specific business process should be initialized as early as possible. Technically the context is created by passing the headers of defined metadata fields to a service that supports the metadata context. No standard component should create a metadata context initially, because the metadata context always depends on the environment. The easiest way to initially create a metadata context is to add the respective headers in a proxy that exposes a service, e.g. an Ingress in Kubernetes. The proxy may derive the metadata information from the used domain, query params, user agent headers and other information available in the request. If getting the metadata information is more complex, an environment specific interaction service (according to the SDA service architecture) may be implemented and create the context programmatically. This service should act like a proxy and forward the request to a standard business service. Services that recreate a context for a process that was interrupted, use the context they stored along with the entity to activate it for current thread.","title":"Creating metadata context information"},{"location":"metadata-context/#support-metadata-context-in-a-service","text":"Any service that communicates synchronously with the HTTP platform clients provided by this library and asynchronously with Kafka consumers and producers initialized by this library and has no persistence layer most likely supports the metadata context. Exceptions may apply if business processes are interrupted or asynchronously processed without saving business data in a database. Services that persist data and therefore interrupt the business process until the data is loaded into memory again can support the metadata context if they store it along with the entity and recreate the runtime context when the entity is loaded and processed. Service that interrupts a process in memory or proceeds asynchronously must transfer the metadata context to the new thread to support the metadata context. Services that fulfill these requirements should mention the support of the metadata context in their deployment documentation. They should also mention the configuration environment variable METADATA_FIELDS that is automatically available for all services based on this library. Services that read information from the metadata context shall not use hard coded field names. They must not expect that specific fields are available in an environment. The environment, technically the operations team, is in the lead to define which metadata is available. A service only knows a specific purpose for which it supports metadata information. Whenever a service supports decisions based on metadata configuration, it must provide a configuration option which naming is based on the purpose to let the operations team define which metadata field should be used. The library supports to derive the actual field names from environment variables. The values are dependent on the environment as well. Therefore, the service must provide configuration options to map values of the metadata field to values it understands. Supporting information from the metadata context to distinguish the behavior of the service can be very complex due to the loose coupling and the high flexibility for the owners of the environment. Every configuration that can be modified by information of the metadata context must have a default as well, because the service may be used in an environment without configured metadata context or in a specific business process is no context available. How a service behaves and is configured when metadata context is available must be described in the deployment documentation to make this complex flexibility manageable by the operations team.","title":"Support metadata context in a service"},{"location":"metadata-context/#example-template-selection","text":"Imagine a business case that is advertised on different landing pages, where the user enters information and receives a result asynchronously via email. The email sent to the user should match the style of the landing page. Each landing page is available on a dedicated domain but all lead to the same backend services. The services may communicate asynchronously or even need additional information from the backoffice team to create the result. The operations team defines the METADATA_FIELDS as landing-page-source for all services in the environment to identify the landing page where the user entered their data. They configure the Ingress, to set either christmas-special or summer-edition as landing-page-source header - based on the domain of two landing pages. No service but the one that is generating the email is affected by this information. The core business rules that determine the result only need the information the user entered. The email content can be created independently as well. Only the service that renders the HTML part of the email must be aware of the source of the request. Therefore, it must have the ability to use multiple templates. Besides other optional input to determine the template to use, it must be configurable to use information from the metadata context. To keep the flexibility the metadata context offers, it defines the environment variable TEMPLATE_HINT_METADATA_FIELD . The operations team configures it as landing-page-source . The service developer can use TEMPLATE_HINT_METADATA_FIELD in the MetadataContext API to get the values of current context, but must be aware that the result is empty or contains multiple values. Additionally, the email templating service must provide configuration options to select a template based on the value as well as providing the templates. The templates and the configuration could be mounted as files in the container. The latter may be a yaml file and look like this: 1 2 3 4 5 6 7 # templates.yaml default : /mounts/templates/default.hbs byMetadata : - metadataValue : christmas-special templateFile : /mounts/templates/xmas.hbs - metadataValue : summer-edition templateFile : /mounts/templates/summer.hbs","title":"Example: Template selection"},{"location":"metadata-context/#variant-of-template-selection","text":"The email example is very flexible and would support other environments as well, e.g. when A/B testing is used to find the perfect color for the use case. The email service is not involved in the evaluation of the A/B test, but has to send emails that match the styles of the A/B test. Such user tests will most likely be evaluated by dedicated UX tools and not by the backend services in the environment. No business service should be affected by this information. However, a service creating statistics for the use case could support the metadata context as well and provide additional evaluation for each variant. The frontend must provide any information in the request to distinguish between A and B. Then the proxy can convert the information in a header that is sent to the backend service, e.g. ab-variant=neon . The operations team of this environment will configure METADATA_FIELDS for all services and TEMPLATE_HINT_METADATA_FIELD as ab-variant and the email templating like this: 1 2 3 4 5 6 7 # templates.yaml default : /mounts/templates/default.hbs byMetadata : - metadataValue : pastel templateFile : /mounts/templates/pastel.hbs - metadataValue : neon templateFile : /mounts/templates/neon.hbs","title":"Variant of template selection"},{"location":"migration-2-to-3/","text":"Migration Guide for Upgrading from 2.x.x to 3.x.x \u00b6 SDA Spring Boot Commons 3 updates Spring Boot from 2.7.x to 3.1.x and Spring Cloud from 2021.x.x to 2022.x.x. This comes with some breaking changes introduced by Spring and some from SDA Spring Boot Commons as well. Other libraries are upgraded or replaced as well. In addition to this migration guide, the official migration guide of Spring Boot should be consulted. Summary of noticeable changes: javax dependencies are replaced by jakarta dependencies. Tracing moved from Sleuth to Open Telemetry. AsyncAPI setup has been changed, as well as the used library to generate Json Schemas from code. Using of the generator is simpler now as described in its documentation . A major upgrade of Spring Security is included. Spring configuration properties changed. S3 libraries changed to a new and different Java API of AWS. WireMock dependency changed to standalone variant that provides transitive dependencies directly in new packages. Compile task \u00b6 Spring Boot 3 requires reflection information about parameters in compiled classes. If no such information is added, warning logs will complain about it. To avoid warnings, the -parameters flag must be added to the compile task. Gradle Maven 1 2 3 4 tasks . withType ( JavaCompile ). configureEach { options . encoding = 'UTF-8' options . compilerArgs << '-parameters' } 1 2 3 4 5 6 7 8 9 10 11 12 13 <build> <plugins> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <configuration> <compilerArgs> <arg> -parameters </arg> </compilerArgs> </configuration> </plugin> </plugins> </build> Jakarta EE \u00b6 Please make sure dependencies do not pull in transitive javax modules and migrate all javax imports to jakarta . The provided dependency management should take care about all dependencies by referring to the dependency management of Spring Boot as mentioned in the migration guide . Please note that Wiremock repackaged javax classes. They will be available in tests but should not be used in your code. Feign Clients with JAX-RS \u00b6 If Feign Clients are declared with JAX-RS annotations like @POST and the feign.jaxrs2.JAXRS2Contract configuration, it must be changed to Jakarta annotations and the feign.jaxrs.JakartaContract configuration or to Spring Boot annotations. Tracing \u00b6 Tracing uses OpenTelemetry now. Any configuration that references zipkin or sleuth is not considered anymore, such as environment variables JAEGER_ENABLED SPRING_ZIPKIN_BASE_URL and SPRING_ZIPKIN_ENABLED . The environment variable JAEGER_SERVICE_NAME was replaced by the spring's default system property spring.application.name . Please refer to the respective management.tracing and other migrated properties in the documentation . AsyncAPI generation \u00b6 Json Schemas for AsyncAPI are generated with Victools' Json Schema Generator now. The previously used library is barely maintained in the past years. The old library provided their own annotations. Now, annotations of Jackson (e.g. @JsonSchemaDescription ), Swagger (e.g. @Schema ) and Jakarta Validation (e.g. NotNull ) can be used. Note that not all attributes of all annotations are covered and multiple examples are not possible anymore. Only one example can be defined with @Schema(example = \"value\") . How the Java classes for schema definitions in the AsyncAPI are defined has changed. Previously, classes to integrate were defined in the code ( .withSchema(\"./schema.json\", BaseEvent.class) ) and referenced in the AsyncAPI template ( $ref: './schema.json#/definitions/CarManufactured' ). Now the classes are referenced directly in the template ( $ref: 'class://com.example.BaseEvent ). The builder method withSchema does not exist any more. Please review the differences in the generated AsyncAPI file. Both libraries work different and have a different feature set. The new generator may have some limitations but a great API for extensions. Please file an issue if something important can't be expressed. MongoDB \u00b6 If you use de.flapdoodle.embed:de.flapdoodle.embed.mongo for testing, change to de.flapdoodle.embed:de.flapdoodle.embed.mongo.spring30x . To define the database version used for tests, use the property de.flapdoodle.mongodb.embedded.version instead of spring.mongodb.embedded.version S3 \u00b6 The simple interface client com.amazonaws.services.s3.AmazonS3 ( Java 1.x), available on version 2.x, was replaced by the service client software.amazon.awssdk.services.s3.S3Client ( Java 2.x) in this version, so you should replace your autowired beans with this new service client and its methods. If you define this bean with custom configuration you need to update your configuration, e.g.: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Configuration public class Foo { - private AmazonS3 createClientWithCustomConfiguration() { - final AWSCredentials credentials = new BasicAWSCredentials(accessKeyId, secretKey); - return AmazonS3ClientBuilder.standard() - .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(endpoint, region)) - .withPathStyleAccessEnabled(true) - .withCredentials(new AWSStaticCredentialsProvider(credentials)) + private S3Client createClientWithCustomConfiguration() { + return S3Client.builder() + .region(Region.of(region)) + .credentialsProvider( + StaticCredentialsProvider.create(AwsBasicCredentials.create(accessKeyId, secretKey))) + .endpointProvider(S3EndpointProvider.defaultProvider()) + .endpointOverride(URI.create(endpoint)) + .forcePathStyle(true) .build(); } } You can check the new API definition here . Kafka \u00b6 There has been introduced a new default value for the sda.kafka.consumer.dlt.pattern property, which is dlt-<topic> . If you want to keep the old behaviour, which is appending .DLT to the consumed topic, you have to explicitly unset the property ( sda.kafka.consumer.dlt.pattern= ). Info We do not recommend using a '.' within the topic name, since it does not allow the standard replacement approach in our kustomize deployments and let to errors in customer's environments. Testing - WireMock \u00b6 WireMock changed from wiremock-jre8 to wiremock-jre8-standalone as suggested here . The standalone variant repackages transitive dependencies to a new package and avoids the dependency. Tests are affected if Jackson classes are used to interact with WireMock, e.g. .willReturn(jsonResponse(jacksonsJsonNode, 200)) . All imports related to Jackson classes interacting with WireMock, including an ObjectMapper that is used to read or write these objects, must be updated from com.fasterxml.\u2026 to wiremock.com.fasterxml.\u2026 .","title":"Migrate to v3"},{"location":"migration-2-to-3/#migration-guide-for-upgrading-from-2xx-to-3xx","text":"SDA Spring Boot Commons 3 updates Spring Boot from 2.7.x to 3.1.x and Spring Cloud from 2021.x.x to 2022.x.x. This comes with some breaking changes introduced by Spring and some from SDA Spring Boot Commons as well. Other libraries are upgraded or replaced as well. In addition to this migration guide, the official migration guide of Spring Boot should be consulted. Summary of noticeable changes: javax dependencies are replaced by jakarta dependencies. Tracing moved from Sleuth to Open Telemetry. AsyncAPI setup has been changed, as well as the used library to generate Json Schemas from code. Using of the generator is simpler now as described in its documentation . A major upgrade of Spring Security is included. Spring configuration properties changed. S3 libraries changed to a new and different Java API of AWS. WireMock dependency changed to standalone variant that provides transitive dependencies directly in new packages.","title":"Migration Guide for Upgrading from 2.x.x to 3.x.x"},{"location":"migration-2-to-3/#compile-task","text":"Spring Boot 3 requires reflection information about parameters in compiled classes. If no such information is added, warning logs will complain about it. To avoid warnings, the -parameters flag must be added to the compile task. Gradle Maven 1 2 3 4 tasks . withType ( JavaCompile ). configureEach { options . encoding = 'UTF-8' options . compilerArgs << '-parameters' } 1 2 3 4 5 6 7 8 9 10 11 12 13 <build> <plugins> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <configuration> <compilerArgs> <arg> -parameters </arg> </compilerArgs> </configuration> </plugin> </plugins> </build>","title":"Compile task"},{"location":"migration-2-to-3/#jakarta-ee","text":"Please make sure dependencies do not pull in transitive javax modules and migrate all javax imports to jakarta . The provided dependency management should take care about all dependencies by referring to the dependency management of Spring Boot as mentioned in the migration guide . Please note that Wiremock repackaged javax classes. They will be available in tests but should not be used in your code.","title":"Jakarta EE"},{"location":"migration-2-to-3/#feign-clients-with-jax-rs","text":"If Feign Clients are declared with JAX-RS annotations like @POST and the feign.jaxrs2.JAXRS2Contract configuration, it must be changed to Jakarta annotations and the feign.jaxrs.JakartaContract configuration or to Spring Boot annotations.","title":"Feign Clients with JAX-RS"},{"location":"migration-2-to-3/#tracing","text":"Tracing uses OpenTelemetry now. Any configuration that references zipkin or sleuth is not considered anymore, such as environment variables JAEGER_ENABLED SPRING_ZIPKIN_BASE_URL and SPRING_ZIPKIN_ENABLED . The environment variable JAEGER_SERVICE_NAME was replaced by the spring's default system property spring.application.name . Please refer to the respective management.tracing and other migrated properties in the documentation .","title":"Tracing"},{"location":"migration-2-to-3/#asyncapi-generation","text":"Json Schemas for AsyncAPI are generated with Victools' Json Schema Generator now. The previously used library is barely maintained in the past years. The old library provided their own annotations. Now, annotations of Jackson (e.g. @JsonSchemaDescription ), Swagger (e.g. @Schema ) and Jakarta Validation (e.g. NotNull ) can be used. Note that not all attributes of all annotations are covered and multiple examples are not possible anymore. Only one example can be defined with @Schema(example = \"value\") . How the Java classes for schema definitions in the AsyncAPI are defined has changed. Previously, classes to integrate were defined in the code ( .withSchema(\"./schema.json\", BaseEvent.class) ) and referenced in the AsyncAPI template ( $ref: './schema.json#/definitions/CarManufactured' ). Now the classes are referenced directly in the template ( $ref: 'class://com.example.BaseEvent ). The builder method withSchema does not exist any more. Please review the differences in the generated AsyncAPI file. Both libraries work different and have a different feature set. The new generator may have some limitations but a great API for extensions. Please file an issue if something important can't be expressed.","title":"AsyncAPI generation"},{"location":"migration-2-to-3/#mongodb","text":"If you use de.flapdoodle.embed:de.flapdoodle.embed.mongo for testing, change to de.flapdoodle.embed:de.flapdoodle.embed.mongo.spring30x . To define the database version used for tests, use the property de.flapdoodle.mongodb.embedded.version instead of spring.mongodb.embedded.version","title":"MongoDB"},{"location":"migration-2-to-3/#s3","text":"The simple interface client com.amazonaws.services.s3.AmazonS3 ( Java 1.x), available on version 2.x, was replaced by the service client software.amazon.awssdk.services.s3.S3Client ( Java 2.x) in this version, so you should replace your autowired beans with this new service client and its methods. If you define this bean with custom configuration you need to update your configuration, e.g.: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Configuration public class Foo { - private AmazonS3 createClientWithCustomConfiguration() { - final AWSCredentials credentials = new BasicAWSCredentials(accessKeyId, secretKey); - return AmazonS3ClientBuilder.standard() - .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(endpoint, region)) - .withPathStyleAccessEnabled(true) - .withCredentials(new AWSStaticCredentialsProvider(credentials)) + private S3Client createClientWithCustomConfiguration() { + return S3Client.builder() + .region(Region.of(region)) + .credentialsProvider( + StaticCredentialsProvider.create(AwsBasicCredentials.create(accessKeyId, secretKey))) + .endpointProvider(S3EndpointProvider.defaultProvider()) + .endpointOverride(URI.create(endpoint)) + .forcePathStyle(true) .build(); } } You can check the new API definition here .","title":"S3"},{"location":"migration-2-to-3/#kafka","text":"There has been introduced a new default value for the sda.kafka.consumer.dlt.pattern property, which is dlt-<topic> . If you want to keep the old behaviour, which is appending .DLT to the consumed topic, you have to explicitly unset the property ( sda.kafka.consumer.dlt.pattern= ). Info We do not recommend using a '.' within the topic name, since it does not allow the standard replacement approach in our kustomize deployments and let to errors in customer's environments.","title":"Kafka"},{"location":"migration-2-to-3/#testing-wiremock","text":"WireMock changed from wiremock-jre8 to wiremock-jre8-standalone as suggested here . The standalone variant repackages transitive dependencies to a new package and avoids the dependency. Tests are affected if Jackson classes are used to interact with WireMock, e.g. .willReturn(jsonResponse(jacksonsJsonNode, 200)) . All imports related to Jackson classes interacting with WireMock, including an ObjectMapper that is used to read or write these objects, must be updated from com.fasterxml.\u2026 to wiremock.com.fasterxml.\u2026 .","title":"Testing - WireMock"},{"location":"security/","text":"Security Hardening \u00b6 sda-spring-boot-commons changes some default configuration for security reasons. This document provides a brief overview about the addressed risks. Risk: Accessing critical resources from untrusted environments \u00b6 To avoid exposing internal resources, Spring Boot Actuator is configured to listen on a separate port. Health, metrics and other sensitive information can't be exposed to the internet by accident, e.g. by missing to exclude the actuator path. Custom critical resources can be exposed at the management port by implementing org.springframework.boot.actuate.endpoint.web.annotation.RestControllerEndpoint or org.springframework.boot.actuate.endpoint.web.annotation.ControllerEndpoint . Note that there is an open discussion about these annotations. As long as they are not deprecated, it is suggested to use them because the use is most similar to controllers used in regular REST APIs. Risk: Root start \u00b6 If the service is started with extended privileges as the root user, an attacker can more easily attack the operating system after taking over from the container. The default configuration is capable to run as no root, listening to ports 8080 and 8081. Deployment checks must ensure, that the container is not configured with a root user. Risk: Exploitation of HTTP methods \u00b6 The HTTP method TRACE is disabled by default to mitigate Cross Site Tracing . Risk: Loss of source IP address \u00b6 We expect, that services built with sda-spring-boot-commons are deployed behind a proxy, e.g. an Ingress in Kubernetes. This library is configured by default to consider X-Forwarded-* headers to identify the original caller. Risk: Detection of confidential components \u00b6 Knowing the components used in a software makes it easier to look for and exploit specific CVEs. Custom error handlers and other configurations are used to avoid identifiable default output from the framework and its components. Risk: Lack of visibility \u00b6 If there is no visibility, there is no response to an abusive action and attackers can explore risks undisturbed. Logs are written to standard out by default to comply with Kubernetes environments. Prometheus metrics are exposed as expected by SDA environments. Risk: Buffer Overflow \u00b6 The size of request and response headers is limited to 8KiB. The size of a request body is limited to 1 MB by default, chunked encoding is not accepted and the Content-Length request header is required. The limit can be changed . Header \u00b6 By configuring the default headers, the following risks are addressed: Cross-Site Scripting Content interpretation by the browser Content loading in Flash and PDFs Clickjacking Sharing visited URLs with third parties Abuse from Cross-Origin Resource Sharing","title":"Security Hardening"},{"location":"security/#security-hardening","text":"sda-spring-boot-commons changes some default configuration for security reasons. This document provides a brief overview about the addressed risks.","title":"Security Hardening"},{"location":"security/#risk-accessing-critical-resources-from-untrusted-environments","text":"To avoid exposing internal resources, Spring Boot Actuator is configured to listen on a separate port. Health, metrics and other sensitive information can't be exposed to the internet by accident, e.g. by missing to exclude the actuator path. Custom critical resources can be exposed at the management port by implementing org.springframework.boot.actuate.endpoint.web.annotation.RestControllerEndpoint or org.springframework.boot.actuate.endpoint.web.annotation.ControllerEndpoint . Note that there is an open discussion about these annotations. As long as they are not deprecated, it is suggested to use them because the use is most similar to controllers used in regular REST APIs.","title":"Risk: Accessing critical resources from untrusted environments"},{"location":"security/#risk-root-start","text":"If the service is started with extended privileges as the root user, an attacker can more easily attack the operating system after taking over from the container. The default configuration is capable to run as no root, listening to ports 8080 and 8081. Deployment checks must ensure, that the container is not configured with a root user.","title":"Risk: Root start"},{"location":"security/#risk-exploitation-of-http-methods","text":"The HTTP method TRACE is disabled by default to mitigate Cross Site Tracing .","title":"Risk: Exploitation of HTTP methods"},{"location":"security/#risk-loss-of-source-ip-address","text":"We expect, that services built with sda-spring-boot-commons are deployed behind a proxy, e.g. an Ingress in Kubernetes. This library is configured by default to consider X-Forwarded-* headers to identify the original caller.","title":"Risk: Loss of source IP address"},{"location":"security/#risk-detection-of-confidential-components","text":"Knowing the components used in a software makes it easier to look for and exploit specific CVEs. Custom error handlers and other configurations are used to avoid identifiable default output from the framework and its components.","title":"Risk: Detection of confidential components"},{"location":"security/#risk-lack-of-visibility","text":"If there is no visibility, there is no response to an abusive action and attackers can explore risks undisturbed. Logs are written to standard out by default to comply with Kubernetes environments. Prometheus metrics are exposed as expected by SDA environments.","title":"Risk: Lack of visibility"},{"location":"security/#risk-buffer-overflow","text":"The size of request and response headers is limited to 8KiB. The size of a request body is limited to 1 MB by default, chunked encoding is not accepted and the Content-Length request header is required. The limit can be changed .","title":"Risk: Buffer Overflow"},{"location":"security/#header","text":"By configuring the default headers, the following risks are addressed: Cross-Site Scripting Content interpretation by the browser Content loading in Flash and PDFs Clickjacking Sharing visited URLs with third parties Abuse from Cross-Origin Resource Sharing","title":"Header"},{"location":"starter-kafka/","text":"Starter Kafka \u00b6 The module sda-commons-starter-kafka provides autoconfigured Kafka producer and consumer configuration. Based on: org.springframework.boot:spring-boot-starter org.springframework.boot:spring-boot-starter-validation org.springframework.kafka:spring-kafka Configuration \u00b6 Property Description Default Example Env sda.kafka.consumer.retry.initialBackOffInterval int The initial backoff of the retry in milli seconds. 1000 1500 SDA_KAFKA_CONSUMER_RETRY_INITIAL_BACKOFF_INTERVALL sda.kafka.consumer.retry.maxBackOffInterval int The max backoff interval in milli seconds. 4000 5000 SDA_KAFKA_CONSUMER_RETRY_MAX_BACKOFF_INTERVALL sda.kafka.consumer.retry.backOffMultiplier double The multiplier beginning with the initial backoff. 2 1.5 SDA_KAFKA_CONSUMER_RETRY_INITIAL_BACKOFF_INTERVALL sda.kafka.consumer.retry.maxRetries int Max retries consuming the offset. 4 10 SDA_KAFKA_CONSUMER_RETRY_INITIAL_MAXRETRIES sda.kafka.consumer.dlt.pattern string Pattern of consumer dead letter topic. <topic> will be replaced by topic name. If not set, \".DLT\" is added to the topic name. \"dlt-<topic>\" \"prefix-<topic>\" SDA_KAFKA_CONSUMER_DLT_PATTERN management.health.kafka.enabled boolean Flag to enable kafka health check. true false MANAGEMENT_HEALTH_KAFKA_ENABLED management.health.kafka.timeout duration Allowed duration for health check to finish. 4s \"5000ms\" MANAGEMENT_HEALTH_KAFKA_TIMEOUT spring.kafka.bootstrap.servers string Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. \"localhost:9092\" \"kafka-broker:9092\" SPRING_KAFKA_BOOTSTRAP_SERVERS spring.kafka.security.protocol string The security protocol used by Kafka. Please note that SASL mechanism requires some manual configuration. \"PLAINTEXT\" \"SSL\" SPRING_KAFKA_SECURITY_PROTOCOL spring.kafka.ssl.keystore-location url Location of the SSL keystore file. \"file:///kafka/kafka.client.keystore.jks\" SPRING_KAFKA_SSL_KEYSTORELOCATION spring.kafka.ssl.key-store-password string Password for the SSL keystore file. \"s3cr3t\" SPRING_KAFKA_SSL_KEYSTOREPASSWORD spring.kafka.ssl.trust-store-location string Location of the SSL truststore file. \"file:/kafka-certs/kafka.client.keystore.jks\" SPRING_KAFKA_SSL_TRUSTSTORELOCATION spring.kafka.ssl.trust-store-password string Password for the SSL truststore file. \"s3cret\" SPRING_KAFKA_SSL_TRUSTSTOREPASSWORD spring.kafka.consumer.group-id string Consumer group name of Kafka Consumer. \"default\" \"my-service-name\" SPRING_KAFKA_CONSUMER_GROUPID Make sure to overwrite spring.kafka.consumer.group-id in your application.properties otherwise you could have conflicts with other services using default . For further information have a look to the Spring Kafka reference documentation . Default configuration set by this library 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 spring.kafka.consumer.group-id = default spring.kafka.consumer.auto-offset-reset = earliest spring.kafka.consumer.key-deserializer = org.springframework.kafka.support.serializer.ErrorHandlingDeserializer spring.kafka.consumer.value-deserializer = org.springframework.kafka.support.serializer.ErrorHandlingDeserializer spring.kafka.consumer.properties.spring.deserializer.key.delegate.class = org.apache.kafka.common.serialization.StringDeserializer spring.kafka.consumer.properties.spring.deserializer.value.delegate.class = org.apache.kafka.common.serialization.ByteArrayDeserializer spring.kafka.consumer.properties.spring.json.trusted.packages = * ### SDA SPECIFIC CONFIGURATION sda.kafka.consumer.retry.initialBackoffInterval = 1000 sda.kafka.consumer.retry.maxBackoffInterval = 4000 sda.kafka.consumer.retry.backoffMultiplier = 2 sda.kafka.consumer.retry.maxRetries = 4 sda.kafka.consumer.dlt.pattern = dlt-<topic> spring.kafka.producer.key-serializer = org.apache.kafka.common.serialization.StringSerializer spring.kafka.producer.value-serializer = org.springframework.kafka.support.serializer.JsonSerializer Consumer configuration \u00b6 The autoconfigured consumer configuration provides several ConcurrentKafkaListenerContainerFactory<String, ?> which can be referenced in @KafkaListener annotated methods. SdaKafkaListenerContainerFactory.LOG_ON_FAILURE Simply logs the exception; with a record listener, the remaining records from the previous poll are passed to the listener. SdaKafkaListenerContainerFactory.RETRY_AND_LOG Skips record that keeps failing after sda.kafka.consumer.retry.maxRetries (default: 4 ) and log exception. SdaKafkaListenerContainerFactory.RETRY_AND_DLT Skips record that keeps failing after sda.kafka.consumer.retry.maxRetries (default: 4) and produces failed record to topic with .DLT suffix. By default, the dead-letter record is sent to a topic named .DLT (the original topic name suffixed with .DLT) and to the same partition as the original record. Therefore, when you use the default configuration, the dead-letter topic must have at least as many partitions as the original topic. The spring default DLT naming convention can be overwritten using the sda.kafka.consumer.dlt.pattern property. The pattern must contain <topic> , which will be replaced by the actual topic name. To skip retry for specific business errors, you can throw the custom NotRetryableKafkaException . Each containerFactory expects a message key as String and the message payload of any type. The payload is deserialized as byte array and converted with the ByteArrayJsonMessageConverter . The ack mode for offsets is per default RECORD where the offset after each record is processed by the listener. 1 2 3 4 5 6 7 8 9 @KafkaListener ( topics = \"TestTopic\" , containerFactory = SdaKafkaListenerContainerFactory . RETRY_AND_LOG ) public void retryAndLog ( @Payload @Valid Message message ) { // doSomething if ( businessError ) { throw new NotRetryableKafkaException (); } } Producer configuration \u00b6 The autoconfigured producer configuration provides a preconfigured KafkaTemplate for producing messages with String key and payload as json . You just need to autowire the KafkaTemplate and you are ready to go.","title":"Starter Kafka"},{"location":"starter-kafka/#starter-kafka","text":"The module sda-commons-starter-kafka provides autoconfigured Kafka producer and consumer configuration. Based on: org.springframework.boot:spring-boot-starter org.springframework.boot:spring-boot-starter-validation org.springframework.kafka:spring-kafka","title":"Starter Kafka"},{"location":"starter-kafka/#configuration","text":"Property Description Default Example Env sda.kafka.consumer.retry.initialBackOffInterval int The initial backoff of the retry in milli seconds. 1000 1500 SDA_KAFKA_CONSUMER_RETRY_INITIAL_BACKOFF_INTERVALL sda.kafka.consumer.retry.maxBackOffInterval int The max backoff interval in milli seconds. 4000 5000 SDA_KAFKA_CONSUMER_RETRY_MAX_BACKOFF_INTERVALL sda.kafka.consumer.retry.backOffMultiplier double The multiplier beginning with the initial backoff. 2 1.5 SDA_KAFKA_CONSUMER_RETRY_INITIAL_BACKOFF_INTERVALL sda.kafka.consumer.retry.maxRetries int Max retries consuming the offset. 4 10 SDA_KAFKA_CONSUMER_RETRY_INITIAL_MAXRETRIES sda.kafka.consumer.dlt.pattern string Pattern of consumer dead letter topic. <topic> will be replaced by topic name. If not set, \".DLT\" is added to the topic name. \"dlt-<topic>\" \"prefix-<topic>\" SDA_KAFKA_CONSUMER_DLT_PATTERN management.health.kafka.enabled boolean Flag to enable kafka health check. true false MANAGEMENT_HEALTH_KAFKA_ENABLED management.health.kafka.timeout duration Allowed duration for health check to finish. 4s \"5000ms\" MANAGEMENT_HEALTH_KAFKA_TIMEOUT spring.kafka.bootstrap.servers string Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. \"localhost:9092\" \"kafka-broker:9092\" SPRING_KAFKA_BOOTSTRAP_SERVERS spring.kafka.security.protocol string The security protocol used by Kafka. Please note that SASL mechanism requires some manual configuration. \"PLAINTEXT\" \"SSL\" SPRING_KAFKA_SECURITY_PROTOCOL spring.kafka.ssl.keystore-location url Location of the SSL keystore file. \"file:///kafka/kafka.client.keystore.jks\" SPRING_KAFKA_SSL_KEYSTORELOCATION spring.kafka.ssl.key-store-password string Password for the SSL keystore file. \"s3cr3t\" SPRING_KAFKA_SSL_KEYSTOREPASSWORD spring.kafka.ssl.trust-store-location string Location of the SSL truststore file. \"file:/kafka-certs/kafka.client.keystore.jks\" SPRING_KAFKA_SSL_TRUSTSTORELOCATION spring.kafka.ssl.trust-store-password string Password for the SSL truststore file. \"s3cret\" SPRING_KAFKA_SSL_TRUSTSTOREPASSWORD spring.kafka.consumer.group-id string Consumer group name of Kafka Consumer. \"default\" \"my-service-name\" SPRING_KAFKA_CONSUMER_GROUPID Make sure to overwrite spring.kafka.consumer.group-id in your application.properties otherwise you could have conflicts with other services using default . For further information have a look to the Spring Kafka reference documentation . Default configuration set by this library 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 spring.kafka.consumer.group-id = default spring.kafka.consumer.auto-offset-reset = earliest spring.kafka.consumer.key-deserializer = org.springframework.kafka.support.serializer.ErrorHandlingDeserializer spring.kafka.consumer.value-deserializer = org.springframework.kafka.support.serializer.ErrorHandlingDeserializer spring.kafka.consumer.properties.spring.deserializer.key.delegate.class = org.apache.kafka.common.serialization.StringDeserializer spring.kafka.consumer.properties.spring.deserializer.value.delegate.class = org.apache.kafka.common.serialization.ByteArrayDeserializer spring.kafka.consumer.properties.spring.json.trusted.packages = * ### SDA SPECIFIC CONFIGURATION sda.kafka.consumer.retry.initialBackoffInterval = 1000 sda.kafka.consumer.retry.maxBackoffInterval = 4000 sda.kafka.consumer.retry.backoffMultiplier = 2 sda.kafka.consumer.retry.maxRetries = 4 sda.kafka.consumer.dlt.pattern = dlt-<topic> spring.kafka.producer.key-serializer = org.apache.kafka.common.serialization.StringSerializer spring.kafka.producer.value-serializer = org.springframework.kafka.support.serializer.JsonSerializer","title":"Configuration"},{"location":"starter-kafka/#consumer-configuration","text":"The autoconfigured consumer configuration provides several ConcurrentKafkaListenerContainerFactory<String, ?> which can be referenced in @KafkaListener annotated methods. SdaKafkaListenerContainerFactory.LOG_ON_FAILURE Simply logs the exception; with a record listener, the remaining records from the previous poll are passed to the listener. SdaKafkaListenerContainerFactory.RETRY_AND_LOG Skips record that keeps failing after sda.kafka.consumer.retry.maxRetries (default: 4 ) and log exception. SdaKafkaListenerContainerFactory.RETRY_AND_DLT Skips record that keeps failing after sda.kafka.consumer.retry.maxRetries (default: 4) and produces failed record to topic with .DLT suffix. By default, the dead-letter record is sent to a topic named .DLT (the original topic name suffixed with .DLT) and to the same partition as the original record. Therefore, when you use the default configuration, the dead-letter topic must have at least as many partitions as the original topic. The spring default DLT naming convention can be overwritten using the sda.kafka.consumer.dlt.pattern property. The pattern must contain <topic> , which will be replaced by the actual topic name. To skip retry for specific business errors, you can throw the custom NotRetryableKafkaException . Each containerFactory expects a message key as String and the message payload of any type. The payload is deserialized as byte array and converted with the ByteArrayJsonMessageConverter . The ack mode for offsets is per default RECORD where the offset after each record is processed by the listener. 1 2 3 4 5 6 7 8 9 @KafkaListener ( topics = \"TestTopic\" , containerFactory = SdaKafkaListenerContainerFactory . RETRY_AND_LOG ) public void retryAndLog ( @Payload @Valid Message message ) { // doSomething if ( businessError ) { throw new NotRetryableKafkaException (); } }","title":"Consumer configuration"},{"location":"starter-kafka/#producer-configuration","text":"The autoconfigured producer configuration provides a preconfigured KafkaTemplate for producing messages with String key and payload as json . You just need to autowire the KafkaTemplate and you are ready to go.","title":"Producer configuration"},{"location":"starter-mongodb/","text":"Starter MongoDB \u00b6 The module sda-commons-starter-mongodb provides several autoconfigured features including: - The Read/Write ZonedDateTime converter - Automatic index creation Based on: - org.springframework.boot:spring-boot-starter-data-mongodb For further documentation please have a look at the Spring Data MongoDB reference documentation . Main Configuration \u00b6 Property Description Default Example Env spring.data.mongodb.uri string The MongoDB connection string . mongodb://localhost:27017/test SPRING_DATA_MONGODB_URI sda.caCertificates.certificatesDir string A directory with CA certificates in PEM format that will be picked up to trust the connection to the database. \"/var/trust/certificates\" \"/my-certs\" SDA_CACERTIFICATES_CERTIFICATESDIR Configuration properties \u00b6 spring.data.mongodb.uri string Mongo database URI. Example: mongodb://exampleUser:examplePassword@mongoHost:27017 Format: mongodb://[username:password@]host1[:port1][,...hostN[:portN]][/[defaultauthdb][?options]] Connection String Options need to be added to the end of the URI e.g. ?ssl=true to enable SSL ?retryWrites=false to disable retryable writes for the connection. ?readPreference=secondaryPreferred In most situations, operations read from secondary members, but in situations where the set consists of a single primary (and no other members), the read operation will use the replica set's primary. For further information take a look on Connection String documentation SSL support \u00b6 The mongodb starter can be configured to use ssl when the option ?ssl=true is used. Certificates in PEM format can be mounted in the directory /var/trust/certificates they will be used by the mongodb client. All certificates found in subdirectories will also be loaded. Note that this directory is also configurable through the property sda.caCertificates.certificatesDir . Testing \u00b6 It is recommended to test with an embedded MongoDB using Flapdoodle's Spring Boot module and the SDA Spring Boot Commons testing module: 1 2 3 4 dependencies { testImplementation 'de.flapdoodle.embed:de.flapdoodle.embed.mongo.spring30x' testImplementation 'org.sdase.commons.spring.boot:sda-commons-web-testing' } Flapdoodle will start a MongoDB server and configures the connection for Spring Boot tests. The MongoDB version can be selected with (test) application properties: 1 de.flapdoodle.mongodb.embedded.version = 4.4.1 MongoOperations can be used to clean the database between tests: 1 2 3 4 5 6 @Autowired MongoOperations mongoOperations ; @BeforeEach void beforeEach () { mongoOperations . dropCollection ( \"collectionUnderTest\" ); } To skip Flapdoodle's autoconfiguration and use a provided database (e.g. an AWS DocumentDB), the property test.mongodb.connection.string (or environment variable TEST_MONGODB_CONNECTION_STRING ) can be used to provide a complete MongoDB Connection String . This feature is provided by the SDA Spring Boot Commons testing module and is only activated when Flapdoodle's autoconfiguration is available for the test setup.","title":"Starter MongoDB"},{"location":"starter-mongodb/#starter-mongodb","text":"The module sda-commons-starter-mongodb provides several autoconfigured features including: - The Read/Write ZonedDateTime converter - Automatic index creation Based on: - org.springframework.boot:spring-boot-starter-data-mongodb For further documentation please have a look at the Spring Data MongoDB reference documentation .","title":"Starter MongoDB"},{"location":"starter-mongodb/#main-configuration","text":"Property Description Default Example Env spring.data.mongodb.uri string The MongoDB connection string . mongodb://localhost:27017/test SPRING_DATA_MONGODB_URI sda.caCertificates.certificatesDir string A directory with CA certificates in PEM format that will be picked up to trust the connection to the database. \"/var/trust/certificates\" \"/my-certs\" SDA_CACERTIFICATES_CERTIFICATESDIR","title":"Main Configuration"},{"location":"starter-mongodb/#configuration-properties","text":"spring.data.mongodb.uri string Mongo database URI. Example: mongodb://exampleUser:examplePassword@mongoHost:27017 Format: mongodb://[username:password@]host1[:port1][,...hostN[:portN]][/[defaultauthdb][?options]] Connection String Options need to be added to the end of the URI e.g. ?ssl=true to enable SSL ?retryWrites=false to disable retryable writes for the connection. ?readPreference=secondaryPreferred In most situations, operations read from secondary members, but in situations where the set consists of a single primary (and no other members), the read operation will use the replica set's primary. For further information take a look on Connection String documentation","title":"Configuration properties"},{"location":"starter-mongodb/#ssl-support","text":"The mongodb starter can be configured to use ssl when the option ?ssl=true is used. Certificates in PEM format can be mounted in the directory /var/trust/certificates they will be used by the mongodb client. All certificates found in subdirectories will also be loaded. Note that this directory is also configurable through the property sda.caCertificates.certificatesDir .","title":"SSL support"},{"location":"starter-mongodb/#testing","text":"It is recommended to test with an embedded MongoDB using Flapdoodle's Spring Boot module and the SDA Spring Boot Commons testing module: 1 2 3 4 dependencies { testImplementation 'de.flapdoodle.embed:de.flapdoodle.embed.mongo.spring30x' testImplementation 'org.sdase.commons.spring.boot:sda-commons-web-testing' } Flapdoodle will start a MongoDB server and configures the connection for Spring Boot tests. The MongoDB version can be selected with (test) application properties: 1 de.flapdoodle.mongodb.embedded.version = 4.4.1 MongoOperations can be used to clean the database between tests: 1 2 3 4 5 6 @Autowired MongoOperations mongoOperations ; @BeforeEach void beforeEach () { mongoOperations . dropCollection ( \"collectionUnderTest\" ); } To skip Flapdoodle's autoconfiguration and use a provided database (e.g. an AWS DocumentDB), the property test.mongodb.connection.string (or environment variable TEST_MONGODB_CONNECTION_STRING ) can be used to provide a complete MongoDB Connection String . This feature is provided by the SDA Spring Boot Commons testing module and is only activated when Flapdoodle's autoconfiguration is available for the test setup.","title":"Testing"},{"location":"starter-s3/","text":"Starter S3 \u00b6 This library provides features for dealing with the Amazon S3 file storage. Based on: - io.awspring.cloud:spring-cloud-aws-core The configuration class contains two beans, namely: AmazonS3Client : Providing an interface for accessing the S3 object storage. S3BucketRepository : Providing an abstraction for s3 client with simple repository methods. Configuration \u00b6 The following properties are needed for the configuration. Property Description Example Env s3.bucketName string The name of the bucket containing the desired object. myphotos S3_BUCKET_NAME s3.endpoint string The endpoint either with or without the protocol https://s3.eu-west-1.amazonaws.com or s3.eu-west-1.amazonaws.com S3_ENDPOINT s3.region string The region to use for SigV4 signing of requests eu-west-1 S3_REGION s3.secretKey string The AWS secret access key s3cret S3_SECRET_KEY s3.accessKey string The AWS access key s3cretAccess S3_ACCESS_KEY","title":"Starter S3"},{"location":"starter-s3/#starter-s3","text":"This library provides features for dealing with the Amazon S3 file storage. Based on: - io.awspring.cloud:spring-cloud-aws-core The configuration class contains two beans, namely: AmazonS3Client : Providing an interface for accessing the S3 object storage. S3BucketRepository : Providing an abstraction for s3 client with simple repository methods.","title":"Starter S3"},{"location":"starter-s3/#configuration","text":"The following properties are needed for the configuration. Property Description Example Env s3.bucketName string The name of the bucket containing the desired object. myphotos S3_BUCKET_NAME s3.endpoint string The endpoint either with or without the protocol https://s3.eu-west-1.amazonaws.com or s3.eu-west-1.amazonaws.com S3_ENDPOINT s3.region string The region to use for SigV4 signing of requests eu-west-1 S3_REGION s3.secretKey string The AWS secret access key s3cret S3_SECRET_KEY s3.accessKey string The AWS access key s3cretAccess S3_ACCESS_KEY","title":"Configuration"},{"location":"starter-web/","text":"Starter Web \u00b6 The module sda-commons-starter-web provides several features to create a service based on the SDA core concepts. Configuration \u00b6 Property Description Default Example Env auth.issuers string Comma separated string of open id discovery key sources with required issuers. https://iam-int.dev.de/auth/realms/123 AUTH_ISSUERS auth.disable boolean Disables authorization checks completely. false true AUTH_DISABLE opa.disable boolean Disables authorization checks with Open Policy Agent completely. In this case access to all resources is granted but no constraints are provided. false true OPA_DISABLE opa.base.url string The baseUrl of the Open Policy Agent Server. \"http://localhost:8181\" \"http://opa-service:8181\" OPA_BASE_URL opa.policy.package string The policy package to check for authorization. The policy must return the property allow as boolean as access decision and may return additional properties as constraints. the package of the application class, be aware that moving the class causes a breaking change regarding deployment if the package is not explicitly set. \"com.custom.package.name\" OPA_POLICY_PACKAGE opa.exclude.patterns string Custom excluded paths can be configured as comma separated list of regex. openapi.json and openapi.yaml \"/customPathOne,/customPathTwo\" OPA_EXCLUDE_PATTERNS opa.client.connection.timeout string The connection timeout of the client that calls the Open Policy Agent server. \"500ms\" \"2s\" OPA_CLIENT_CONNECTION_TIMEOUT opa.client.timeout string The read timeout of the client that calls the Open Policy Agent server. \"500ms\" \"2s\" OPA_CLIENT_TIMEOUT oidc.client.enabled boolean Enables OIDC Authentication (Client Credentials Flow) for the configured clients. false true OIDC_CLIENT_ENABLED oidc.client.id string The client ID for the registration. \"exampleClient\" OPA_CLIENT_ID oid.client.secret string The Client secret of the registration. \"s3cret\" OIDC_CLIENT_SECRET oidc.client.issuer.uri string URI that can either be an OpenID Connect discovery endpoint or an OAuth 2.0 Authorization Server Metadata endpoint defined by RFC 8414. \"https://keycloak.sdadev.sda-se.io/auth/realms/exampleRealm\" OIDC_CLIENT_ISSUER_URI cors.allowed-origin-patterns string Comma separated list of URL patterns for which CORS requests are allowed. none allowed \"https://*.all-subdomains.com, https://static-domain.com\" CORS_ALLOWEDORIGINPATTERNS request.body.max.size size The maximum size allowed for request body data sent by a client. 1 MB 100 KB , 10MB REQUEST_BODY_MAX_SIZE enable.json.logging boolean If logs should be printed as JSON. Note: This config param is not available for application.properties or application.yaml false true ENABLE_JSON_LOGGING management.otlp.tracing.endpoint string Base url to OTLP Collector instance. It applies for http or gRPC, if enabled http://grafana-agent-traces.monitoring:4317 \"http://localhost:4318\" MANAGEMENT_OTLP_TRACING_ENDPOINT management.tracing.enabled boolean If tracing should be unit-tested, it is important to have the annotation @AutoConfigureObservability on your test class to enable tracing. true ( false in test contexts) false MANAGEMENT_TRACING_ENABLED management.tracing.sampling.probability number Probability in the range from 0.0 to 1.0 that a trace will be sampled. 1.0 0.2 MANAGEMENT_TRACING_SAMPLING_PROBABILITY management.tracing.propagation.type string Tracing context propagation types produced and consumed by the application. Setting this property overrides the more fine-grained propagation type properties. \"b3,w3c\" \"b3\" MANAGEMENT_TRACING_PROPAGATION_TYPE management.tracing.grpc.enabled boolean You only need to set this property to true if you want to use grpc (port 4317) vs http (port 4318) channel for span export. true false MANAGEMENT_TRACING_GRPC_ENABLED management.otlp.tracing.compression string Method used to compress the payload. Options: \"gzip\", \"none\" \"none\" \"gzip\" MANAGEMENT_OTLP_TRACING_COMPRESSION management.otlp.tracing.timeout string Call timeout for the OTel Collector to process an exported batch of data. This timeout spans the entire call. \"10s\" \"20s\" MANAGEMENT_OTLP_TRACING_TIMEOUT spring.application.name string The application name, also used for tracing. For PR deployments, you need to make sure it will have the PR suffix to distinguish tracings. \"application\" \"my-service-name\" SPRING_APPLICATION_NAME For further information have a look at the Spring Boot documentation . Default configuration set by this library 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 server.servlet.context-path = /api server.port = 8080 server.max-http-request-header-size = 8192 server.error.whitelabel.enabled = false server.tomcat.basedir = ${java.io.tmpdir}/tomcat request.body.max.size = 1 MB # Actuator management.server.port = 8081 management.server.base-path = / management.endpoints.web.base-path = / management.endpoints.web.exposure.include = * management.endpoints.enabled-by-default = false # Healthcheck management.endpoint.health.enabled = true management.endpoints.web.path-mapping.health = healthcheck management.endpoint.health.probes.enabled = true # Add the required auto-configured health indicators which are supported in org.sdase.commons.spring # See https://docs.spring.io/spring-boot/docs/current/reference/html/actuator.html#actuator.endpoints.health.auto-configured-health-indicators # to see the available indicators. If an included HealthIndicator is not autoconfigured, it will be automatically ignored (see management.endpoint.health.validate-group-membership) management.endpoint.health.group.readiness.include = readinessState, mongo, openPolicyAgent, kafka # since 3.1.0, configured health checks must exist, # see https://github.com/spring-projects/spring-boot/commit/c55d398f95bf1c64a55ea95e1dc8ae20e9ce7561#diff-ecf768cadbb11cdb6a8999f942301ff33662b2b00221188613ab3c4402e1200a management.endpoint.health.validate-group-membership = false management.endpoint.health.show-details = always management.endpoint.health.show-components = always # Metrics management.endpoints.web.path-mapping.prometheus = metrics/prometheus management.metrics.web.server.request.autotime.enabled = true management.endpoint.prometheus.enabled = true management.endpoint.metrics.enabled = true management.tracing.enabled = true management.tracing.propagation.type = b3,w3c management.tracing.sampling.probability = 1.0 management.otlp.tracing.endpoint = http://grafana-agent-traces.monitoring:4317 management.otlp.tracing.compression = none management.otlp.tracing.timeout = 10s management.tracing.grpc.enabled = true feign.metrics.enabled = true Please make sure to configure spring.application.name for every service. Authentication \u00b6 Spring Security Documentation Enables feature that make a Spring Boot service compliant with the SDA SE Authentication concepts using OIDC. OIDC Authentication can be configured with auth.issuers to provide a comma separated list of trusted issuers. In develop and test environments, the boolean auth.disable may be used to disable authentication. The JWKS URI of each issuer is updated when an unknown Key ID is received and every 5 minutes. The cache of known JWK is invalidated after 15 minutes. Warning This setup allows authenticated and anonymous requests! It is the responsibility of policies provided by the Open Policy Agent to decide about denying anonymous requests. Spring Security is disabled for the Management/Admin Port (default: 8081). Be aware that these port should not be accessible out of the deployment context. This security implementation lacks some features compared to sda-dropwizard-commons : - No configuration of static local public keys to verify the token signature. - No configuration of JWKS URIs to verify the token signature. - The IDP must provide an iss claim that matches the base URI for discovery. - Leeway is not configurable yet. - The client that loads the JWKS is not configurable yet. Authorization \u00b6 Enables feature that make a Spring Boot service compliant with the SDA SE Authorization concepts using Open Policy Agent. The authorization is done by the Open Policy Agent . It can be configured with opa. configuration properties . Requests to the Open Policy Agent Requests to the server are determined by the base URL and the policy package. Given the default base URL http://localhost:8181 and an example package of com.my.service , the Open Policy Agent server will be requested for authorization decision at http://localhost:8181/v1/data/com/my/service . The OPA configuration acts as a client to the Open Policy Agent and is hooked in as request filter which is part of the SecurityFilterChain including the OIDC Authentication. Constraints provided with the Open Policy Agent response can be mapped to a custom POJO. The class must extend org.sdase.commons.spring.boot.web.auth.opa.AbstractConstraints and must be annotated with org.sdase.commons.spring.boot.web.auth.opa.Constraints . It has request scope and can be @Autowired in @Controllers or @RestControllers . Custom Constraint implementation com.example.my.service.rego MyConstraints.java MyController.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package com . example . my . service import input # decode the token token = { \"payload\" : payload } { not input . jwt = null io . jwt . decode ( input . jwt , [ _ , payload , _ ]) } # deny by default! default allow = false allow { # your rules } admin { token . payload . admin = \"yes\" } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Constraints public class MyConstraints extends AbstractConstraints { private boolean admin ; public MyConstraints setAdmin ( boolean admin ) { this . admin = admin ; return this ; } public boolean isAdmin () { return admin ; } } 1 2 3 4 5 6 @RestController public class MyController { @Autowired private MyConstraints myConstraints ; // ... } Additional parameters that are needed for the authorization decision may be provided with custom org.sdase.commons.spring.boot.web.auth.opa.extension.OpaInputExtension s. Testing \u00b6 The testing module provides aligned test dependencies including Wiremock for external APIs and JUnit extensions to mock or disable authentication and authorization. OPA \u00b6 The OPA configuration requests the policy decision providing the following inputs Property Description Example input.httpMethod HTTP method as uppercase string. \"GET\" input.path Requested path as array of path segments without context or servlet path. [\"myResource\", \"123-id\", \"someSubresource\"] input.jwt Validated encoded JWT as string (if available). \"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiIxIn0.Xpk63zUfXiI5f_bdGjqrhx03aGyBn9ETgXbkAgLalPk\" input.headers All HTTP request headers as object with lower-case header names as key and array of headers as value. {\"accept\": \"text/plain\", \"accept\": \"application/json\"} Security note While a service might only consider one value of a specific header, that a policy might authorize on an array of those or vice versa. Consider this in your policy when you want to make sure you authorize on the same value that a service might use to evaluate the output. Remark to HTTP request headers The configuration normalizes header names to lower case to simplify handling in OPA since HTTP specification defines header names as case-insensitive. Multivalued headers are not normalized with respect to the representation as list or single string with separator char. They are forwarded as parsed by the framework. Example Policy Using Input Policy Result 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # each policy lies in a package that is referenced in the configuration of the OpaBundle package example # decode the JWT as new variable 'token' token = { \"payload\" : payload } { not input . jwt == null io . jwt . decode ( input . jwt , [ _ , payload , _ ]) } # deny by default default allow = false # the allow property is required for authorization allow { # allow if path match '/contracts/:anyid' input . path = [ \"contracts\" , _ ] # allow if request method 'GET' is used input . httpMethod == \"GET\" # allow if 'claim' exists in the JWT payload token . payload . claim # allow if a request header 'HttpRequestHeaderName' has a certain value input . headers [ \"httprequestheadername\" ][ _ ] == \"certain-value\" } # set some example constraints # constraints are always service dependent and the structure, if any, is defined in each service constraint1 := true # always true constraint2 := [ \"v2.1\" , \"v2.2\" ] # always an array of \"v2.1\" and \"v2.2\" constraint3 [ token . payload . sub ] # always a set that contains the 'sub' claim from the token # or is empty if no token is present 1 2 3 4 5 6 7 8 { \"result\" : { \"allow\" : true , \"constraint1\" : true , \"constraint2\" : [ \"v2.1\" , \"v2.2\" ], \"constraint3\" : [ \"my-sub\" ] } } Http Client \u00b6 Enables support for org.springframework.cloud.openfeign.FeignClients that support SDA Platform features like: - passing the Authorization header to downstream services. - passing the Trace-Token header to downstream services. - OIDC client authentication A feign client can be created as interface like this: 1 2 3 4 5 @FeignClient ( name = \"partnerOds\" , url = \"${partnerOds.baseUrl}\" ) public interface OtherServiceClient { @GetMapping ( \"/partners\" ) List < Partner > getPartners (); } Then the spring boot application needs to be annotated with @EnableFeignClients in order for the component scanning to pick up the @FeignClient annotated interfaces like so 1 2 3 4 @EnableFeignClients @SpringBootApplication public class ExampleApplication { (...) } The Partner ODS base url must be configured as http://partner-ods:8080/api in the Spring environment property partnerOds.baseUrl . Detailed configuration like timeouts can be configured with default feign properties in the application.yaml or derived environment properties based on the name attribute of the org.springframework.cloud.openfeign.FeignClient annotation. The client is then available as bean in the Spring context. Authentication forwarding \u00b6 The client can be used within the SDA Platform to path through the received authentication header by adding a configuration: 1 2 3 4 5 6 7 8 9 @FeignClient ( name = \"partnerOds\" , url = \"${partnerOds.baseUrl}\" , configuration = { AuthenticationPassThroughClientConfiguration . class } ) public interface OtherServiceClient { @GetMapping ( \"/partners\" ) List < Partner > getPartners (); } org.sdase.commons.spring.boot.web.client.AuthenticationPassThroughClientConfiguration will take the Authorization header from the current request context of the servlet and adds its value to the client request. Trace-Token \u00b6 The client can be used within the SDA Platform to pass through the received Trace-Token header by adding a configuration: 1 2 3 4 5 6 7 8 9 @FeignClient ( name = \"partnerOds\" , url = \"${partnerOds.baseUrl}\" , configuration = { SdaTraceTokenClientConfiguration . class } ) public interface OtherServiceClient { @GetMapping ( \"/partners\" ) List < Partner > getPartners (); } org.sdase.commons.spring.boot.web.tracing.SdaTraceTokenClientConfiguration will take the Trace-Token header from the current request context of the servlet and adds its value to the client request. If no Trace-Token header is present in the current request context, the SdaTraceTokenClientConfiguration will generate a new Trace-Token and pass it to the following requests. OIDC Client \u00b6 If the request context is not always existing, e.g. in cases where a technical user for service-to-service communication is required, the .org.sdase.commons.spring.boot.web.client.OidcClientRequestConfiguration will request the required OIDC authentication token with the client credentials flow using the configured \"oidc.client.issuer.uri\" , \"oidc.client.id\" and \"oidc.client.secret\" . If the current request context contains the Authorization header, the authentication pass-through will be applied instead. JAX-RS Mapping \u00b6 If you would like to use JAX-RS based web annotations, you just need to apply the feign.jaxrs.JakartaContract.class to configurations. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Path ( \"customers\" ) @FeignClient ( value = \"customerService\" , url = \"${customer.api.base.url}\" , configuration = { OidcClientRequestConfiguration . class , feign . jaxrs . JakartaContract . class }) public interface CustomerServiceApi { @POST @Path ( \"/{customerId}/contracts\" ) @Consumes ( APPLICATION_JSON ) void addContract ( @PathParam ( \"customerId\" ) @NotBlank String customerId , Contract contract ); } Platform Client \u00b6 The PlatformClient combines the authentication forwarding, trace token and OIDC configuration without the need to configure each individually. 1 2 3 4 5 6 7 @PlatformClient ( value = \"customerService\" , url = \"${customer.api.base.url}\" ) public interface CustomerServiceApi { // ... } It abstracts some configuration of the FeignClient and is then available as bean as well. Error Handling \u00b6 The module sda-commons-starter-web provides a shared ApiError model, to provide a common response error structure for SDA-restful services. Usage \u00b6 Per default, the module sda-commons-starter-web autoconfigures a global @ExceptionHandler(ApiException.class) as @ControllerAdvice . As a result, the exception handler is per default provided to every @Controller . Referencing in OpenAPI \u00b6 To provide the common ApiError in the API, you need to reference the class as @Schema . 1 2 3 4 5 @ApiResponse( responseCode = \"422\", description = \"The request could not be processed due to invalid parameters. Details are provided in the error response.\", content = @Content(schema = @Schema(implementation = ApiError.class))) Throwing ApiException \u00b6 When the ApiException is thrown the @ExceptionHandler automatically intercepts the exception and maps the related ResponseEntity . As the result, the controller returns the related http response code and the nested ApiError . 1 2 3 4 5 6 throw ApiException.builder() .httpCode(422) .title(\"Invalid input\") .detail(\"name\", \"name was not null\", \"NOT_NULL\") .cause(e) .build(); In this example the controller would return with http status 422 and body: 1 2 3 4 5 6 7 8 9 10 { \"title\" : \"Invalid input\" , \"invalidParams\" : [ { \"field\" : \"name\" , \"reason\" : \"name was not null\" , \"errorCode\" : \"NOT_NULL\" } ] } Async \u00b6 The default Spring async task executor is autoconfigured to transfer the request attributes of the current request to the Thread running the asynchronous method. Jackson \u00b6 Enables feature that makes a Spring Boot service compliant with the REST guide of SDA SE. So far this covers: - the tolerant reader pattern - consistent serialization of java.time.ZonedDateTime compatible to the type date-time of JSON-Schema . It is strongly recommended to use - java.time.LocalDate for dates without time serialized as 2018-09-23 - java.time.ZonedDateTime for date and times serialized as 2018-09-23T14:21:41+01:00 - java.time.Duration for durations with time resolution serialized as P1DT13M - java.time.Period for durations with day resolution serialized as P1Y2D All these types can be read and written in JSON as ISO 8601 formats. Reading java.time.ZonedDateTime is configured to be tolerant so that added nanoseconds or missing milliseconds or missing seconds are supported. @com.fasterxml.jackson.annotation.JsonFormat(pattern = \"...\") should not be used for customizing serialization because it breaks tolerant reading of formatting variants. If a specific field should be serialized with milliseconds, it must be annotated with @com.fasterxml.jackson.databind.annotation.JsonSerialize(using = Iso8601Serializer.WithMillis.class) . If a specific field should be serialized with nanoseconds, it must be annotated with @com.fasterxml.jackson.databind.annotation.JsonSerialize(using = Iso8601Serializer.WithNanos.class) Differences to the known SDA Dropwizard Commons configuration - java.time.ZonedDateTime fields are serialized with seconds by default. There is no other global configuration for java.time.ZonedDateTime serialization available. - Fewer modules are activated for foreign frameworks . Compared to SDA Dropwizard Commons, GuavaExtrasModule, JodaModule, and CaffeineModule are not registered anymore. - No documented customization of the global com.fasterxml.jackson.databind.ObjectMapper is available right now. - Support for HAL Links and embedding linked resources is not implemented. - Support for YAML is not implemented. - There is no support for field filters . Such filters have been barely used in the SDA SE. Monitoring \u00b6 Services use Prometheus to scrape and store metrics. An actuator exposing metrics in prometheus format is available using the following endpoint 1 http://{serviceURL}:{adminPort}/metrics/prometheus Spring Boot is using micrometer to instrument code using out-of-the-box bindings for common libraries. SDA specific metrics \u00b6 Metric Name Labels Description healthcheck_status name Exposes healthcheck as metric for multiple indicators JVM and System metrics \u00b6 Metric Name Labels Description jvm_classes_loaded_classes The number of classes that are currently loaded in the Java virtual machine. jvm_classes_unloaded_classes The total number of classes unloaded since the Java virtual machine has started execution. jvm_buffer_count_buffers id An estimate of the number of buffers in the pool. jvm_buffer_memory_used_bytes id An estimate of the memory that the Java virtual machine is using for this buffer pool. jvm_buffer_total_capacity_bytes id An estimate of the total capacity of the buffers in this pool. jvm_memory_used_bytes id , area The amount of used memory. jvm_memory_committed_bytes id , area The amount of memory in bytes that is committed for the Java virtual machine to use. jvm_memory_max_bytes id , area The maximum amount of memory in bytes that can be used for memory management. jvm_gc_max_data_size_bytes Max size of long-lived heap memory pool. jvm_gc_live_data_size_bytes Size of long-lived heap memory pool after reclamation. jvm_gc_memory_allocated_bytes_total Incremented for an increase in the size of the (young) heap memory pool after one GC to before the next. jvm_gc_memory_promoted_bytes_total Count of positive increases in the size of the old generation memory pool before GC to after GC. jvm_gc_concurrent_phase_time_seconds_count gc , action , cause Time spent in concurrent phase. jvm_gc_concurrent_phase_time_seconds_sum gc , action , cause jvm_gc_concurrent_phase_time_seconds_max gc , action , cause jvm_gc_pause_seconds_count gc , action , cause Time spent in GC pause. jvm_gc_pause_seconds_sum gc , action , cause jvm_gc_pause_seconds_max gc , action , cause system_cpu_count The number of processors available to the Java virtual machine. system_load_average_1m The sum of the number of runnable entities queued to available processors and the number of runnable entities running on the available processors averaged over a period of time. system_cpu_usage The \"recent cpu usage\" of the system the application is running in. process_cpu_usage The \"recent cpu usage\" for the Java Virtual Machine process. jvm_threads_peak_threads The peak live thread count since the Java virtual machine started or peak was reset. jvm_threads_daemon_threads The current number of live daemon threads. jvm_threads_live The current number of live threads including both daemon and non-daemon threads. jvm_threads_started_threads_total The total number of application threads started in the JVM. jvm_threads_states_threads state The current number of threads. Key Metrics for monitoring Kafka \u00b6 Metric name Labels Description kafka_producer_compression_rate_avg client.id The average compression rate of record batches, defined as the average ratio of the compressed batch size over the uncompressed size. kafka_producer_response_rate client.id The number of responses received per second kafka_producer_request_rate client.id The number of requests sent per second kafka_producer_request_latency_avg client.id The average request latency in ms kafka_producer_outgoing_byte_rate client.id The number of outgoing bytes sent to all servers per second kafka_producer_io_wait_time_ns_avg client.id The average length of time the I/O thread spent waiting for a socket ready for reads or writes in nanoseconds. kafka_producer_batch_size_avg client.id The average number of bytes sent per partition per-request. kafka_consumer_records_lag client.id Number of messages consumer is behind producer on this partition kafka_consumer_records_lag_max client.id Maximum number of messages consumer is behind producer, either for a specific partition or across all partitions on this client kafka_consumer_bytes_consumed_rate client.id Average number of bytes consumed per second for a specific topic or across all topics. kafka_consumer_records_consumed_rate client.id Average number of records consumed per second for a specific topic or across all topics kafka_consumer_fetch_rate client.id Number of fetch requests per second from the consumer MongoDB metrics \u00b6 Metric name Labels Description mongodb_driver_pool_waitqueuesize cluster_id , server_address The current size of the wait queue for a connection from the pool mongodb_driver_pool_checkedout cluster_id , server_address The count of connections that are currently in use mongodb_driver_pool_size cluster_id , server_address The current size of the connection pool, including idle and and in-use members mongodb_driver_commands_seconds_max cluster_id , server_address , collection , command , status Timer of mongodb commands mongodb_driver_commands_seconds_count cluster_id , server_address , collection , command , status Timer of mongodb commands mongodb_driver_commands_seconds_sum cluster_id , server_address , collection , command , status Timer of mongodb commands Tracing \u00b6 Currently, tracing is leveraged by Micrometer Tracing rand OpenTelemetry in the Spring context. OpenTelemetry (OTEL) is a collection of standardized vendor-agnostic tools, APIs, and SDKs. It's a CNCF incubating project and is a merger of the OpenTracing and OpenCensus projects. OpenTracing is a vendor-neutral API for sending telemetry data over to an observability backend. It uses Micrometer for code instrumentation & provide tracing bridge to OpenTelemetry and OpenTelemetry for tools to collect and send telemetry data to the reporter/collector. Default features are: Adds trace and span ids to the Slf4J MDC, so you can extract all the logs from a given trace or span in a log aggregator. Instruments common ingress and egress points from Spring applications (servlet filter, rest template, scheduled actions, message channels, feign client). The service name is derived from spring.application.name Generate and report OTLP traces via HTTP or gRPC. By default, it sends them to a OTLP compatible collector (e.g. Jaeger) on localhost (http port 4317, gRPC port 4318). Configure the location of the service using management.otlp.tracing.endpoint . See above for more common options. You can check all the possible values on OtlpProperties and TracingProperties Health Checks / Actuator \u00b6 Configures the Spring Boot Actuator to be accessible on root path / at default management port 8081 . The following endpoints are provided at the admin management endpoint: Liveness: http://{serviceURL}:{adminPort}/healthcheck/liveness Readiness: http://{serviceURL}:{adminPort}/healthcheck/readiness The readiness group contains the following indicators: ReadinessStateHealthIndicator MongoHealthIndicator , if auto-configured. OpenPolicyAgentHealthIndicator if OPA is enabled for authentication To overwrite the defaults HealthIndicator of the readiness group, you can overwrite the property source: 1 management.endpoint.health.group.readiness.include = readinessState, customCheck Custom health indicators can be easily added to the application context: 1 2 3 4 5 6 7 @Component public class CustomHealthIndicator implements HealthIndicator { @Override public Health health () { return new Health . Builder (). up (). build (); } } The custom health indicator will be available under /healthcheck/custom which is resolved by the prefix of the HealthIndicator implementing component. Logging \u00b6 The Spring Boot default logging is enabled. Logs are printed to standard out. ENABLE_JSON_LOGGING=true as environment variable or -Denable.json.logging=true as JVM parameter enables output as JSON for structured logs used in log aggregation tools. To enable JSON logging in application.(properties/yaml) , logging.config=classpath:org/sdase/commons/spring/boot/web/logging/logback-json.xml may be used. Metadata Context \u00b6 If you want to make use of the data in the metadata context, you should read the dedicated documentation . If your service is required to support the metadata context but is not interested in the data, continue here: Services that use the sda-spring-boot-commons: can access the current org.sdase.commons.spring.boot.metadata.context.MetadataContext in their implementation will automatically load the context from incoming HTTP requests into the thread, handling the request, if you register org.sdase.commons.spring.boot.web.metadata.MetadataContextConfiguration will automatically load the context from consumed Kafka messages into the thread handling the message and the error when handling the message fails when the consumer is configured with one of the provided org.sdase.commons.spring.boot.kafka.SdaKafkaConsumerConfiguration will automatically propagate the context to other services via HTTP when using a org.sdase.commons.spring.boot.web.client.PlatformClient : 1 2 3 4 5 6 7 8 @PlatformClient ( value = \"name\" , url = \"http://your-api-url\" }) public interface ClientWithMetadataConfiguration { @GetMapping ( \"/metadata-hello\" ) Object getSomething (); } when using a FeignClient, that behaviour can be achieved by using the org.sdase.commons.spring.boot.web.metadata.MetadataContextClientConfiguration configuration. 1 2 3 4 5 6 7 8 9 10 @FeignClient ( value = \"name\" , url = \"http://your-api-url\" , configuration = { MetadataContextClientConfiguration . class }) public interface ClientWithMetadataConfiguration { @GetMapping ( \"/metadata-hello\" ) Object getSomething (); } will automatically propagate the context in produced Kafka messages when the producer is created with org.sdase.commons.spring.boot.kafka.SdaKafkaProducerConfiguration are configurable by the property or environment variable METADATA_FIELDS to be aware of the metadata used in a specific environment Services that interrupt a business process should persist the context from MetadataContext.detachedCurrent() and restore it with MetadataContext.createContext(\u2026) when the process continues. Interrupting a business process means that processing is stopped and continued later in a new thread or even another instance of the service. Most likely, this will happen when a business entity is stored based on a request and loaded later for further processing by a scheduler or due to a new user interaction. In this case, the DetachedMetadataContext must be persisted along with the entity and recreated when the entity is loaded. The DetachedMetadataContext can be defined as field in any MongoDB entity. For services that handle requests or messages in parallel, the metadata context attributes will be automatically transferred to the new threads, if @Async is used.","title":"Starter Web"},{"location":"starter-web/#starter-web","text":"The module sda-commons-starter-web provides several features to create a service based on the SDA core concepts.","title":"Starter Web"},{"location":"starter-web/#configuration","text":"Property Description Default Example Env auth.issuers string Comma separated string of open id discovery key sources with required issuers. https://iam-int.dev.de/auth/realms/123 AUTH_ISSUERS auth.disable boolean Disables authorization checks completely. false true AUTH_DISABLE opa.disable boolean Disables authorization checks with Open Policy Agent completely. In this case access to all resources is granted but no constraints are provided. false true OPA_DISABLE opa.base.url string The baseUrl of the Open Policy Agent Server. \"http://localhost:8181\" \"http://opa-service:8181\" OPA_BASE_URL opa.policy.package string The policy package to check for authorization. The policy must return the property allow as boolean as access decision and may return additional properties as constraints. the package of the application class, be aware that moving the class causes a breaking change regarding deployment if the package is not explicitly set. \"com.custom.package.name\" OPA_POLICY_PACKAGE opa.exclude.patterns string Custom excluded paths can be configured as comma separated list of regex. openapi.json and openapi.yaml \"/customPathOne,/customPathTwo\" OPA_EXCLUDE_PATTERNS opa.client.connection.timeout string The connection timeout of the client that calls the Open Policy Agent server. \"500ms\" \"2s\" OPA_CLIENT_CONNECTION_TIMEOUT opa.client.timeout string The read timeout of the client that calls the Open Policy Agent server. \"500ms\" \"2s\" OPA_CLIENT_TIMEOUT oidc.client.enabled boolean Enables OIDC Authentication (Client Credentials Flow) for the configured clients. false true OIDC_CLIENT_ENABLED oidc.client.id string The client ID for the registration. \"exampleClient\" OPA_CLIENT_ID oid.client.secret string The Client secret of the registration. \"s3cret\" OIDC_CLIENT_SECRET oidc.client.issuer.uri string URI that can either be an OpenID Connect discovery endpoint or an OAuth 2.0 Authorization Server Metadata endpoint defined by RFC 8414. \"https://keycloak.sdadev.sda-se.io/auth/realms/exampleRealm\" OIDC_CLIENT_ISSUER_URI cors.allowed-origin-patterns string Comma separated list of URL patterns for which CORS requests are allowed. none allowed \"https://*.all-subdomains.com, https://static-domain.com\" CORS_ALLOWEDORIGINPATTERNS request.body.max.size size The maximum size allowed for request body data sent by a client. 1 MB 100 KB , 10MB REQUEST_BODY_MAX_SIZE enable.json.logging boolean If logs should be printed as JSON. Note: This config param is not available for application.properties or application.yaml false true ENABLE_JSON_LOGGING management.otlp.tracing.endpoint string Base url to OTLP Collector instance. It applies for http or gRPC, if enabled http://grafana-agent-traces.monitoring:4317 \"http://localhost:4318\" MANAGEMENT_OTLP_TRACING_ENDPOINT management.tracing.enabled boolean If tracing should be unit-tested, it is important to have the annotation @AutoConfigureObservability on your test class to enable tracing. true ( false in test contexts) false MANAGEMENT_TRACING_ENABLED management.tracing.sampling.probability number Probability in the range from 0.0 to 1.0 that a trace will be sampled. 1.0 0.2 MANAGEMENT_TRACING_SAMPLING_PROBABILITY management.tracing.propagation.type string Tracing context propagation types produced and consumed by the application. Setting this property overrides the more fine-grained propagation type properties. \"b3,w3c\" \"b3\" MANAGEMENT_TRACING_PROPAGATION_TYPE management.tracing.grpc.enabled boolean You only need to set this property to true if you want to use grpc (port 4317) vs http (port 4318) channel for span export. true false MANAGEMENT_TRACING_GRPC_ENABLED management.otlp.tracing.compression string Method used to compress the payload. Options: \"gzip\", \"none\" \"none\" \"gzip\" MANAGEMENT_OTLP_TRACING_COMPRESSION management.otlp.tracing.timeout string Call timeout for the OTel Collector to process an exported batch of data. This timeout spans the entire call. \"10s\" \"20s\" MANAGEMENT_OTLP_TRACING_TIMEOUT spring.application.name string The application name, also used for tracing. For PR deployments, you need to make sure it will have the PR suffix to distinguish tracings. \"application\" \"my-service-name\" SPRING_APPLICATION_NAME For further information have a look at the Spring Boot documentation . Default configuration set by this library 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 server.servlet.context-path = /api server.port = 8080 server.max-http-request-header-size = 8192 server.error.whitelabel.enabled = false server.tomcat.basedir = ${java.io.tmpdir}/tomcat request.body.max.size = 1 MB # Actuator management.server.port = 8081 management.server.base-path = / management.endpoints.web.base-path = / management.endpoints.web.exposure.include = * management.endpoints.enabled-by-default = false # Healthcheck management.endpoint.health.enabled = true management.endpoints.web.path-mapping.health = healthcheck management.endpoint.health.probes.enabled = true # Add the required auto-configured health indicators which are supported in org.sdase.commons.spring # See https://docs.spring.io/spring-boot/docs/current/reference/html/actuator.html#actuator.endpoints.health.auto-configured-health-indicators # to see the available indicators. If an included HealthIndicator is not autoconfigured, it will be automatically ignored (see management.endpoint.health.validate-group-membership) management.endpoint.health.group.readiness.include = readinessState, mongo, openPolicyAgent, kafka # since 3.1.0, configured health checks must exist, # see https://github.com/spring-projects/spring-boot/commit/c55d398f95bf1c64a55ea95e1dc8ae20e9ce7561#diff-ecf768cadbb11cdb6a8999f942301ff33662b2b00221188613ab3c4402e1200a management.endpoint.health.validate-group-membership = false management.endpoint.health.show-details = always management.endpoint.health.show-components = always # Metrics management.endpoints.web.path-mapping.prometheus = metrics/prometheus management.metrics.web.server.request.autotime.enabled = true management.endpoint.prometheus.enabled = true management.endpoint.metrics.enabled = true management.tracing.enabled = true management.tracing.propagation.type = b3,w3c management.tracing.sampling.probability = 1.0 management.otlp.tracing.endpoint = http://grafana-agent-traces.monitoring:4317 management.otlp.tracing.compression = none management.otlp.tracing.timeout = 10s management.tracing.grpc.enabled = true feign.metrics.enabled = true Please make sure to configure spring.application.name for every service.","title":"Configuration"},{"location":"starter-web/#authentication","text":"Spring Security Documentation Enables feature that make a Spring Boot service compliant with the SDA SE Authentication concepts using OIDC. OIDC Authentication can be configured with auth.issuers to provide a comma separated list of trusted issuers. In develop and test environments, the boolean auth.disable may be used to disable authentication. The JWKS URI of each issuer is updated when an unknown Key ID is received and every 5 minutes. The cache of known JWK is invalidated after 15 minutes. Warning This setup allows authenticated and anonymous requests! It is the responsibility of policies provided by the Open Policy Agent to decide about denying anonymous requests. Spring Security is disabled for the Management/Admin Port (default: 8081). Be aware that these port should not be accessible out of the deployment context. This security implementation lacks some features compared to sda-dropwizard-commons : - No configuration of static local public keys to verify the token signature. - No configuration of JWKS URIs to verify the token signature. - The IDP must provide an iss claim that matches the base URI for discovery. - Leeway is not configurable yet. - The client that loads the JWKS is not configurable yet.","title":"Authentication"},{"location":"starter-web/#authorization","text":"Enables feature that make a Spring Boot service compliant with the SDA SE Authorization concepts using Open Policy Agent. The authorization is done by the Open Policy Agent . It can be configured with opa. configuration properties . Requests to the Open Policy Agent Requests to the server are determined by the base URL and the policy package. Given the default base URL http://localhost:8181 and an example package of com.my.service , the Open Policy Agent server will be requested for authorization decision at http://localhost:8181/v1/data/com/my/service . The OPA configuration acts as a client to the Open Policy Agent and is hooked in as request filter which is part of the SecurityFilterChain including the OIDC Authentication. Constraints provided with the Open Policy Agent response can be mapped to a custom POJO. The class must extend org.sdase.commons.spring.boot.web.auth.opa.AbstractConstraints and must be annotated with org.sdase.commons.spring.boot.web.auth.opa.Constraints . It has request scope and can be @Autowired in @Controllers or @RestControllers . Custom Constraint implementation com.example.my.service.rego MyConstraints.java MyController.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package com . example . my . service import input # decode the token token = { \"payload\" : payload } { not input . jwt = null io . jwt . decode ( input . jwt , [ _ , payload , _ ]) } # deny by default! default allow = false allow { # your rules } admin { token . payload . admin = \"yes\" } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Constraints public class MyConstraints extends AbstractConstraints { private boolean admin ; public MyConstraints setAdmin ( boolean admin ) { this . admin = admin ; return this ; } public boolean isAdmin () { return admin ; } } 1 2 3 4 5 6 @RestController public class MyController { @Autowired private MyConstraints myConstraints ; // ... } Additional parameters that are needed for the authorization decision may be provided with custom org.sdase.commons.spring.boot.web.auth.opa.extension.OpaInputExtension s.","title":"Authorization"},{"location":"starter-web/#testing","text":"The testing module provides aligned test dependencies including Wiremock for external APIs and JUnit extensions to mock or disable authentication and authorization.","title":"Testing"},{"location":"starter-web/#opa","text":"The OPA configuration requests the policy decision providing the following inputs Property Description Example input.httpMethod HTTP method as uppercase string. \"GET\" input.path Requested path as array of path segments without context or servlet path. [\"myResource\", \"123-id\", \"someSubresource\"] input.jwt Validated encoded JWT as string (if available). \"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiIxIn0.Xpk63zUfXiI5f_bdGjqrhx03aGyBn9ETgXbkAgLalPk\" input.headers All HTTP request headers as object with lower-case header names as key and array of headers as value. {\"accept\": \"text/plain\", \"accept\": \"application/json\"} Security note While a service might only consider one value of a specific header, that a policy might authorize on an array of those or vice versa. Consider this in your policy when you want to make sure you authorize on the same value that a service might use to evaluate the output. Remark to HTTP request headers The configuration normalizes header names to lower case to simplify handling in OPA since HTTP specification defines header names as case-insensitive. Multivalued headers are not normalized with respect to the representation as list or single string with separator char. They are forwarded as parsed by the framework. Example Policy Using Input Policy Result 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # each policy lies in a package that is referenced in the configuration of the OpaBundle package example # decode the JWT as new variable 'token' token = { \"payload\" : payload } { not input . jwt == null io . jwt . decode ( input . jwt , [ _ , payload , _ ]) } # deny by default default allow = false # the allow property is required for authorization allow { # allow if path match '/contracts/:anyid' input . path = [ \"contracts\" , _ ] # allow if request method 'GET' is used input . httpMethod == \"GET\" # allow if 'claim' exists in the JWT payload token . payload . claim # allow if a request header 'HttpRequestHeaderName' has a certain value input . headers [ \"httprequestheadername\" ][ _ ] == \"certain-value\" } # set some example constraints # constraints are always service dependent and the structure, if any, is defined in each service constraint1 := true # always true constraint2 := [ \"v2.1\" , \"v2.2\" ] # always an array of \"v2.1\" and \"v2.2\" constraint3 [ token . payload . sub ] # always a set that contains the 'sub' claim from the token # or is empty if no token is present 1 2 3 4 5 6 7 8 { \"result\" : { \"allow\" : true , \"constraint1\" : true , \"constraint2\" : [ \"v2.1\" , \"v2.2\" ], \"constraint3\" : [ \"my-sub\" ] } }","title":"OPA"},{"location":"starter-web/#http-client","text":"Enables support for org.springframework.cloud.openfeign.FeignClients that support SDA Platform features like: - passing the Authorization header to downstream services. - passing the Trace-Token header to downstream services. - OIDC client authentication A feign client can be created as interface like this: 1 2 3 4 5 @FeignClient ( name = \"partnerOds\" , url = \"${partnerOds.baseUrl}\" ) public interface OtherServiceClient { @GetMapping ( \"/partners\" ) List < Partner > getPartners (); } Then the spring boot application needs to be annotated with @EnableFeignClients in order for the component scanning to pick up the @FeignClient annotated interfaces like so 1 2 3 4 @EnableFeignClients @SpringBootApplication public class ExampleApplication { (...) } The Partner ODS base url must be configured as http://partner-ods:8080/api in the Spring environment property partnerOds.baseUrl . Detailed configuration like timeouts can be configured with default feign properties in the application.yaml or derived environment properties based on the name attribute of the org.springframework.cloud.openfeign.FeignClient annotation. The client is then available as bean in the Spring context.","title":"Http Client"},{"location":"starter-web/#authentication-forwarding","text":"The client can be used within the SDA Platform to path through the received authentication header by adding a configuration: 1 2 3 4 5 6 7 8 9 @FeignClient ( name = \"partnerOds\" , url = \"${partnerOds.baseUrl}\" , configuration = { AuthenticationPassThroughClientConfiguration . class } ) public interface OtherServiceClient { @GetMapping ( \"/partners\" ) List < Partner > getPartners (); } org.sdase.commons.spring.boot.web.client.AuthenticationPassThroughClientConfiguration will take the Authorization header from the current request context of the servlet and adds its value to the client request.","title":"Authentication forwarding"},{"location":"starter-web/#trace-token","text":"The client can be used within the SDA Platform to pass through the received Trace-Token header by adding a configuration: 1 2 3 4 5 6 7 8 9 @FeignClient ( name = \"partnerOds\" , url = \"${partnerOds.baseUrl}\" , configuration = { SdaTraceTokenClientConfiguration . class } ) public interface OtherServiceClient { @GetMapping ( \"/partners\" ) List < Partner > getPartners (); } org.sdase.commons.spring.boot.web.tracing.SdaTraceTokenClientConfiguration will take the Trace-Token header from the current request context of the servlet and adds its value to the client request. If no Trace-Token header is present in the current request context, the SdaTraceTokenClientConfiguration will generate a new Trace-Token and pass it to the following requests.","title":"Trace-Token"},{"location":"starter-web/#oidc-client","text":"If the request context is not always existing, e.g. in cases where a technical user for service-to-service communication is required, the .org.sdase.commons.spring.boot.web.client.OidcClientRequestConfiguration will request the required OIDC authentication token with the client credentials flow using the configured \"oidc.client.issuer.uri\" , \"oidc.client.id\" and \"oidc.client.secret\" . If the current request context contains the Authorization header, the authentication pass-through will be applied instead.","title":"OIDC Client"},{"location":"starter-web/#jax-rs-mapping","text":"If you would like to use JAX-RS based web annotations, you just need to apply the feign.jaxrs.JakartaContract.class to configurations. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Path ( \"customers\" ) @FeignClient ( value = \"customerService\" , url = \"${customer.api.base.url}\" , configuration = { OidcClientRequestConfiguration . class , feign . jaxrs . JakartaContract . class }) public interface CustomerServiceApi { @POST @Path ( \"/{customerId}/contracts\" ) @Consumes ( APPLICATION_JSON ) void addContract ( @PathParam ( \"customerId\" ) @NotBlank String customerId , Contract contract ); }","title":"JAX-RS Mapping"},{"location":"starter-web/#platform-client","text":"The PlatformClient combines the authentication forwarding, trace token and OIDC configuration without the need to configure each individually. 1 2 3 4 5 6 7 @PlatformClient ( value = \"customerService\" , url = \"${customer.api.base.url}\" ) public interface CustomerServiceApi { // ... } It abstracts some configuration of the FeignClient and is then available as bean as well.","title":"Platform Client"},{"location":"starter-web/#error-handling","text":"The module sda-commons-starter-web provides a shared ApiError model, to provide a common response error structure for SDA-restful services.","title":"Error Handling"},{"location":"starter-web/#usage","text":"Per default, the module sda-commons-starter-web autoconfigures a global @ExceptionHandler(ApiException.class) as @ControllerAdvice . As a result, the exception handler is per default provided to every @Controller .","title":"Usage"},{"location":"starter-web/#referencing-in-openapi","text":"To provide the common ApiError in the API, you need to reference the class as @Schema . 1 2 3 4 5 @ApiResponse( responseCode = \"422\", description = \"The request could not be processed due to invalid parameters. Details are provided in the error response.\", content = @Content(schema = @Schema(implementation = ApiError.class)))","title":"Referencing in OpenAPI"},{"location":"starter-web/#throwing-apiexception","text":"When the ApiException is thrown the @ExceptionHandler automatically intercepts the exception and maps the related ResponseEntity . As the result, the controller returns the related http response code and the nested ApiError . 1 2 3 4 5 6 throw ApiException.builder() .httpCode(422) .title(\"Invalid input\") .detail(\"name\", \"name was not null\", \"NOT_NULL\") .cause(e) .build(); In this example the controller would return with http status 422 and body: 1 2 3 4 5 6 7 8 9 10 { \"title\" : \"Invalid input\" , \"invalidParams\" : [ { \"field\" : \"name\" , \"reason\" : \"name was not null\" , \"errorCode\" : \"NOT_NULL\" } ] }","title":"Throwing ApiException"},{"location":"starter-web/#async","text":"The default Spring async task executor is autoconfigured to transfer the request attributes of the current request to the Thread running the asynchronous method.","title":"Async"},{"location":"starter-web/#jackson","text":"Enables feature that makes a Spring Boot service compliant with the REST guide of SDA SE. So far this covers: - the tolerant reader pattern - consistent serialization of java.time.ZonedDateTime compatible to the type date-time of JSON-Schema . It is strongly recommended to use - java.time.LocalDate for dates without time serialized as 2018-09-23 - java.time.ZonedDateTime for date and times serialized as 2018-09-23T14:21:41+01:00 - java.time.Duration for durations with time resolution serialized as P1DT13M - java.time.Period for durations with day resolution serialized as P1Y2D All these types can be read and written in JSON as ISO 8601 formats. Reading java.time.ZonedDateTime is configured to be tolerant so that added nanoseconds or missing milliseconds or missing seconds are supported. @com.fasterxml.jackson.annotation.JsonFormat(pattern = \"...\") should not be used for customizing serialization because it breaks tolerant reading of formatting variants. If a specific field should be serialized with milliseconds, it must be annotated with @com.fasterxml.jackson.databind.annotation.JsonSerialize(using = Iso8601Serializer.WithMillis.class) . If a specific field should be serialized with nanoseconds, it must be annotated with @com.fasterxml.jackson.databind.annotation.JsonSerialize(using = Iso8601Serializer.WithNanos.class) Differences to the known SDA Dropwizard Commons configuration - java.time.ZonedDateTime fields are serialized with seconds by default. There is no other global configuration for java.time.ZonedDateTime serialization available. - Fewer modules are activated for foreign frameworks . Compared to SDA Dropwizard Commons, GuavaExtrasModule, JodaModule, and CaffeineModule are not registered anymore. - No documented customization of the global com.fasterxml.jackson.databind.ObjectMapper is available right now. - Support for HAL Links and embedding linked resources is not implemented. - Support for YAML is not implemented. - There is no support for field filters . Such filters have been barely used in the SDA SE.","title":"Jackson"},{"location":"starter-web/#monitoring","text":"Services use Prometheus to scrape and store metrics. An actuator exposing metrics in prometheus format is available using the following endpoint 1 http://{serviceURL}:{adminPort}/metrics/prometheus Spring Boot is using micrometer to instrument code using out-of-the-box bindings for common libraries.","title":"Monitoring"},{"location":"starter-web/#sda-specific-metrics","text":"Metric Name Labels Description healthcheck_status name Exposes healthcheck as metric for multiple indicators","title":"SDA specific metrics"},{"location":"starter-web/#jvm-and-system-metrics","text":"Metric Name Labels Description jvm_classes_loaded_classes The number of classes that are currently loaded in the Java virtual machine. jvm_classes_unloaded_classes The total number of classes unloaded since the Java virtual machine has started execution. jvm_buffer_count_buffers id An estimate of the number of buffers in the pool. jvm_buffer_memory_used_bytes id An estimate of the memory that the Java virtual machine is using for this buffer pool. jvm_buffer_total_capacity_bytes id An estimate of the total capacity of the buffers in this pool. jvm_memory_used_bytes id , area The amount of used memory. jvm_memory_committed_bytes id , area The amount of memory in bytes that is committed for the Java virtual machine to use. jvm_memory_max_bytes id , area The maximum amount of memory in bytes that can be used for memory management. jvm_gc_max_data_size_bytes Max size of long-lived heap memory pool. jvm_gc_live_data_size_bytes Size of long-lived heap memory pool after reclamation. jvm_gc_memory_allocated_bytes_total Incremented for an increase in the size of the (young) heap memory pool after one GC to before the next. jvm_gc_memory_promoted_bytes_total Count of positive increases in the size of the old generation memory pool before GC to after GC. jvm_gc_concurrent_phase_time_seconds_count gc , action , cause Time spent in concurrent phase. jvm_gc_concurrent_phase_time_seconds_sum gc , action , cause jvm_gc_concurrent_phase_time_seconds_max gc , action , cause jvm_gc_pause_seconds_count gc , action , cause Time spent in GC pause. jvm_gc_pause_seconds_sum gc , action , cause jvm_gc_pause_seconds_max gc , action , cause system_cpu_count The number of processors available to the Java virtual machine. system_load_average_1m The sum of the number of runnable entities queued to available processors and the number of runnable entities running on the available processors averaged over a period of time. system_cpu_usage The \"recent cpu usage\" of the system the application is running in. process_cpu_usage The \"recent cpu usage\" for the Java Virtual Machine process. jvm_threads_peak_threads The peak live thread count since the Java virtual machine started or peak was reset. jvm_threads_daemon_threads The current number of live daemon threads. jvm_threads_live The current number of live threads including both daemon and non-daemon threads. jvm_threads_started_threads_total The total number of application threads started in the JVM. jvm_threads_states_threads state The current number of threads.","title":"JVM and System metrics"},{"location":"starter-web/#key-metrics-for-monitoring-kafka","text":"Metric name Labels Description kafka_producer_compression_rate_avg client.id The average compression rate of record batches, defined as the average ratio of the compressed batch size over the uncompressed size. kafka_producer_response_rate client.id The number of responses received per second kafka_producer_request_rate client.id The number of requests sent per second kafka_producer_request_latency_avg client.id The average request latency in ms kafka_producer_outgoing_byte_rate client.id The number of outgoing bytes sent to all servers per second kafka_producer_io_wait_time_ns_avg client.id The average length of time the I/O thread spent waiting for a socket ready for reads or writes in nanoseconds. kafka_producer_batch_size_avg client.id The average number of bytes sent per partition per-request. kafka_consumer_records_lag client.id Number of messages consumer is behind producer on this partition kafka_consumer_records_lag_max client.id Maximum number of messages consumer is behind producer, either for a specific partition or across all partitions on this client kafka_consumer_bytes_consumed_rate client.id Average number of bytes consumed per second for a specific topic or across all topics. kafka_consumer_records_consumed_rate client.id Average number of records consumed per second for a specific topic or across all topics kafka_consumer_fetch_rate client.id Number of fetch requests per second from the consumer","title":"Key Metrics for monitoring Kafka"},{"location":"starter-web/#mongodb-metrics","text":"Metric name Labels Description mongodb_driver_pool_waitqueuesize cluster_id , server_address The current size of the wait queue for a connection from the pool mongodb_driver_pool_checkedout cluster_id , server_address The count of connections that are currently in use mongodb_driver_pool_size cluster_id , server_address The current size of the connection pool, including idle and and in-use members mongodb_driver_commands_seconds_max cluster_id , server_address , collection , command , status Timer of mongodb commands mongodb_driver_commands_seconds_count cluster_id , server_address , collection , command , status Timer of mongodb commands mongodb_driver_commands_seconds_sum cluster_id , server_address , collection , command , status Timer of mongodb commands","title":"MongoDB metrics"},{"location":"starter-web/#tracing","text":"Currently, tracing is leveraged by Micrometer Tracing rand OpenTelemetry in the Spring context. OpenTelemetry (OTEL) is a collection of standardized vendor-agnostic tools, APIs, and SDKs. It's a CNCF incubating project and is a merger of the OpenTracing and OpenCensus projects. OpenTracing is a vendor-neutral API for sending telemetry data over to an observability backend. It uses Micrometer for code instrumentation & provide tracing bridge to OpenTelemetry and OpenTelemetry for tools to collect and send telemetry data to the reporter/collector. Default features are: Adds trace and span ids to the Slf4J MDC, so you can extract all the logs from a given trace or span in a log aggregator. Instruments common ingress and egress points from Spring applications (servlet filter, rest template, scheduled actions, message channels, feign client). The service name is derived from spring.application.name Generate and report OTLP traces via HTTP or gRPC. By default, it sends them to a OTLP compatible collector (e.g. Jaeger) on localhost (http port 4317, gRPC port 4318). Configure the location of the service using management.otlp.tracing.endpoint . See above for more common options. You can check all the possible values on OtlpProperties and TracingProperties","title":"Tracing"},{"location":"starter-web/#health-checks-actuator","text":"Configures the Spring Boot Actuator to be accessible on root path / at default management port 8081 . The following endpoints are provided at the admin management endpoint: Liveness: http://{serviceURL}:{adminPort}/healthcheck/liveness Readiness: http://{serviceURL}:{adminPort}/healthcheck/readiness The readiness group contains the following indicators: ReadinessStateHealthIndicator MongoHealthIndicator , if auto-configured. OpenPolicyAgentHealthIndicator if OPA is enabled for authentication To overwrite the defaults HealthIndicator of the readiness group, you can overwrite the property source: 1 management.endpoint.health.group.readiness.include = readinessState, customCheck Custom health indicators can be easily added to the application context: 1 2 3 4 5 6 7 @Component public class CustomHealthIndicator implements HealthIndicator { @Override public Health health () { return new Health . Builder (). up (). build (); } } The custom health indicator will be available under /healthcheck/custom which is resolved by the prefix of the HealthIndicator implementing component.","title":"Health Checks / Actuator"},{"location":"starter-web/#logging","text":"The Spring Boot default logging is enabled. Logs are printed to standard out. ENABLE_JSON_LOGGING=true as environment variable or -Denable.json.logging=true as JVM parameter enables output as JSON for structured logs used in log aggregation tools. To enable JSON logging in application.(properties/yaml) , logging.config=classpath:org/sdase/commons/spring/boot/web/logging/logback-json.xml may be used.","title":"Logging"},{"location":"starter-web/#metadata-context","text":"If you want to make use of the data in the metadata context, you should read the dedicated documentation . If your service is required to support the metadata context but is not interested in the data, continue here: Services that use the sda-spring-boot-commons: can access the current org.sdase.commons.spring.boot.metadata.context.MetadataContext in their implementation will automatically load the context from incoming HTTP requests into the thread, handling the request, if you register org.sdase.commons.spring.boot.web.metadata.MetadataContextConfiguration will automatically load the context from consumed Kafka messages into the thread handling the message and the error when handling the message fails when the consumer is configured with one of the provided org.sdase.commons.spring.boot.kafka.SdaKafkaConsumerConfiguration will automatically propagate the context to other services via HTTP when using a org.sdase.commons.spring.boot.web.client.PlatformClient : 1 2 3 4 5 6 7 8 @PlatformClient ( value = \"name\" , url = \"http://your-api-url\" }) public interface ClientWithMetadataConfiguration { @GetMapping ( \"/metadata-hello\" ) Object getSomething (); } when using a FeignClient, that behaviour can be achieved by using the org.sdase.commons.spring.boot.web.metadata.MetadataContextClientConfiguration configuration. 1 2 3 4 5 6 7 8 9 10 @FeignClient ( value = \"name\" , url = \"http://your-api-url\" , configuration = { MetadataContextClientConfiguration . class }) public interface ClientWithMetadataConfiguration { @GetMapping ( \"/metadata-hello\" ) Object getSomething (); } will automatically propagate the context in produced Kafka messages when the producer is created with org.sdase.commons.spring.boot.kafka.SdaKafkaProducerConfiguration are configurable by the property or environment variable METADATA_FIELDS to be aware of the metadata used in a specific environment Services that interrupt a business process should persist the context from MetadataContext.detachedCurrent() and restore it with MetadataContext.createContext(\u2026) when the process continues. Interrupting a business process means that processing is stopped and continued later in a new thread or even another instance of the service. Most likely, this will happen when a business entity is stored based on a request and loaded later for further processing by a scheduler or due to a new user interaction. In this case, the DetachedMetadataContext must be persisted along with the entity and recreated when the entity is loaded. The DetachedMetadataContext can be defined as field in any MongoDB entity. For services that handle requests or messages in parallel, the metadata context attributes will be automatically transferred to the new threads, if @Async is used.","title":"Metadata Context"}]}